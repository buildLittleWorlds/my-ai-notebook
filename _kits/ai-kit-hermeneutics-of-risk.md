---
layout: kit
title: "AI and the Hermeneutics of Risk: New Forms of Dialogue and Danger"
kit_type: ai
---
<div class="top-links">

<a href="{{ '/kits/ai-hermeneutics-kit/' | relative_url }}" class="quickkit-pill">← Back to AI
Hermeneutics Kit</a>

</div>


As the preceding pages have shown, the ideas that an author can be
absent, that a text can "read" you, and that a book can be "dangerous"
are all classical concepts in the history of hermeneutics. Artificial
intelligence does not invent these problems, but it does introduce new
forms and intensities of them. A clear-eyed hermeneutics of AI requires
us to distinguish between the traditional risks of interpretation and
the novel risks posed by a computational medium.

------------------------------------------------------------------------

<div class="section" markdown="1">

## The Asymmetric Dialogue: From Metaphorical to Computational Reading

The "dialogue" with a traditional text, as described by Gadamer, is a
metaphor for a transformative intellectual and emotional experience. The
text acts upon you, but it does not know you. The interaction with a
large language model is different in kind. It is an *asymmetric
dialogue*:

- You interpret the AI's output.
- Simultaneously, the AI system is computationally interpreting *you*.
  It processes your prompts, analyzes your language, and builds a
  predictive model of your intentions, knowledge, and needs in order to
  generate its next response.

While a book changes you through a metaphorical conversation, an AI
changes you through a literal feedback loop. This crucial
difference—between being metaphorically understood and computationally
modeled—is the source of the new risks associated with AI.

</div>

<div class="section" markdown="1">

## Two Categories of Hermeneutic Risk

We can organize the potential dangers of AI into two distinct
categories, each relating to a different direction of the interpretive
act.

### 1. The Risks of Reading AI-Generated Texts

These are risks that arise from our interpretation of the machine's
output. They are often intensified versions of the dangers posed by any
persuasive text.

- **Sophisticated Misinformation:** The "dangerous book" tradition warns
  of texts that spread falsehoods. AI can generate such texts (e.g.,
  propaganda, fake news) with unprecedented scale and personalization,
  making them potentially more effective and harder to identify.
- **Cultural Homogenization:** An LLM, as a [](post37.html) "blurry JPEG
  of the web" (Ted Chiang), tends to reproduce the most statistically
  common patterns from its training data. A risk of over-reliance on
  such texts is a potential flattening of linguistic and cultural
  diversity, reinforcing a global monoculture.
- **Hidden Bias:** The training data for large models contains the
  collected biases of human society. AI texts can reproduce and amplify
  these biases (racism, sexism, etc.) under a veneer of objective,
  machine-generated neutrality. Reading such texts uncritically risks
  internalizing these harmful ideologies. This calls for a "hermeneutics
  of suspicion," as advocated by Paul Ricoeur, to unmask these latent
  structures.

### 2. The Risks of Being Read by an AI System

These are risks that arise from the system's interpretation of us. They
are unique to interactive, computational media.

- **Data Extraction and Surveillance:** Our interactions with an AI are
  data. This data can be stored, analyzed, and used for purposes unknown
  to us. This model of data capture has been described by scholars like
  Shoshana Zuboff as "surveillance capitalism," where personal
  experience is treated as a free raw material.
- **Reduction and Categorization:** To process your input, an AI must
  categorize you and your request. The risk is that this computational
  model of you is a radical simplification of your personhood. You are
  reduced to a set of predictable patterns, and your unique identity is
  flattened into a type.
- **Algorithmic Shaping of the Self:** As the AI models you, it tailors
  its responses to what it predicts you want or need. This creates a
  feedback loop that can subtly shape your thoughts, desires, and even
  your sense of self. This aligns with Michel Foucault's concept of
  "technologies of the self," where modern forms of power operate by
  shaping individuals from within.

A complete hermeneutics for the age of AI needs to be a dual practice: a
critical reading of the machine's words, and a critical awareness of how
the machine is, in turn, reading us.

</div>

<div class="bottom-links">

<a href="{{ '/kits/ai-hermeneutics-kit/' | relative_url }}" class="quickkit-pill">← Back to AI
Hermeneutics Kit</a>

</div>
