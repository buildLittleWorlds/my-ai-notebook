URL: https://www.youtube.com/watch?v=LPrGqdidHGM&t=19s
==============================
Channel Title: Elan Barenholtz, PhD
Video Title: Memory is an Illusion | Professor Elan Barenholtz
Publish Date: April 03, 2025
==============================

TRANSCRIPT:

if you spend a lot of time with large language models like I do you can definitely be under the impression that they know a bunch of stuff that they have all kinds of information they even remember previous conversations that they're kind of facts that live within them but the truth is if you think about the architecture of these things there's there's no sense in which you could say there's knowledge in there not in the conventional sense like on a computer where there's a file that you access that contains the information the way large language models work is they are trained uh to just do something called Next token generation if you give them a sequence so the sequence could be you know a question like uh what is it like to be an llm and what they have learned to do is to predict the very next token word uh for all intensive purpose we could just say word and so uh what it like to be an LM it's going to try predict the next word maybe it's as and then uh you take that stick it onto the sequence again and feed it again what what is leg be LM as what what would be the next token prediction and then it could be ASN or then you can get ASN llm you daisy chain them together um and you get this uh think this autor regressive next token generation you keep doing it iterate and then uh you can get very meaningful responses responses that are highly consistent with uh the kind of things we say that people would say if they knew things uh if they remembered things and so the output is consistent with the system knowing all all kinds of information about just about anything you want to talk about whether it's uh you know a recipe for lentil soup or uh String Theory uh these things can speak knowledgeably and pretty competently about all of these things and yet if we think about what the weights contain what what the actual model has within it it's just next token prediction there's no sense in which it knows the long string of ideas that come spewing out of it when you run it autor regressively and yet it's able to produce these sequences now the twist is of course that I'm going to argue is that we are also just autoagressive NEX token generators and that means we don't really have the knowledge we think we do when we recall facts and information we're not really recalling them they don't live in our mind waiting to be accessed like a file on a computer instead what we have is a predisposition to generate certain kinds of sequences um if you ask me uh you know uh who's my favorite musician or something um I don't know if I want to answer that right now uh but let's just say I I love suan Stevens okay he's a I a big fan um was that fact about me loving sufian Stevens him being my favorite musician sort of already there in my brain well yes in the sense that if you ask me the question I'm going to give you that response maybe you know 99 out of 100 uh maybe 100% of the time uh so I'm going to have that presupposition but if we are LMS what I really have is just that next token generative uh predisposition to say that and there's no sense in which I could say that piece of information lives in my mind independent of just the sequence that I generate in response to that question so I'm not exactly sure what that means uh in relation to is is how illusory is our our general conception of memory I think this applies to in some uh maybe more intuitive uh or less intuitive cases like thinking about a memory of an event or some fact like uh you know where do I leave my keys oh I remember they're on the kitchen table and it could feel like I'm just retrieving that fact like there's a stored kind of recording or whatever it is of me leaving those keys on my table but what I'm proposing is really going on is when I think about the question where did I leave my keys my brain cues up a visual memory a sequence of me uh putting my keys on my table and I see that and I observe that that generative Genera sequence and L beh hold I'm like okay boom that's where my keys are and so again it can seem like you're ACC like I'm accessing some sort of stored visual sequence whereas what I'm really doing is just running the next token generation in sequence I'm I'm just generating on demand in that moment so to me this really does kind of run against the grain of of how it feels intuitively to remember something um to to have information to have knowledge uh even beliefs I would argue something more like this predisposition rather than articulated beliefs they can't live in cold storage if it's just this autor regressive kind of process then that's what it is there's no sense in which there's an articulated belief there's just a predisposition to articulated belief or maybe predisposition to exhibit a certain Behavior so that's what I'm uh thinking about right now I'm very curious to hear people's reaction to that and um if people have a uh critique uh an alternative uh perspective reasons why they think this is crazy I'd love to hear them