URL: https://www.youtube.com/watch?v=E9QWvmrWPZE&t=724s
==============================
Channel Title: Elan Barenholtz, PhD
Video Title: Is your Brain a Large Language Model?  Conversation with Prof. William Hahn and Addy Cha
Publish Date: June 19, 2025
==============================

TRANSCRIPT:

All right. What's up everyone? We're back with another Polymath Salon today. This is actually on Alain's channel, not mine, the echalopto on the on Alain's channel. And we're going to be talking about, I suppose, how humans and LMs are not truly so different at the end of the day. Maybe Alain wants to open with a a quick uh summary about that whole thing for those who haven't heard it. Sure. Thanks, uh, Eddie. And this is kind of exciting. So basically what I've been doing for the past year or so is is pursuing this idea that our own language human language is perfectly modeled by the underlying engine of large language models. Meaning that that's the full account uh in terms of our language production language generation that when I'm talking now what that is is basically an LLM and when you're what you're listening with uh is essentially an LLM. that gets a little more complicated when you talk about the receiving end. And so language human language is essentially modeled by an a large by the core engine of large language models. Now we without getting into granular details I don't necessarily mean the exact way it's implemented not even at a software level. um doesn't necessarily have to depend on uh the exact uh specific algorithm uh as to how you know chatbt actually operates but there's something fundamental uh there's a couple of ideas as to as to what's fundamental um and in a nutshell the ideas are that it's self-generating that the structure of language is what generates the next token or uh it doesn't have to be the next token but is what generates language uh so that's the underlying structure itself just relations between words. Um, and that's what generates language that it's uh autogenerative meaning that you can actually uh extract the generative structure from sort of the static structure the geometry of the language. Um, and the second idea is that it's it's auto reggressive. Uh, and that's how chatbt works. It's how a lot of the large language models work. Um, and that's the idea that we are generating those tokens these these next words. um we're doing it sort of auto reggressively meaning we we generate one uh in response to a sequence then tack it onto the sequence and then uh use that new sequence with the next uh the next token next word uh as the input sequence um and so that we're doing it churning through because one for like for all intents and purposes we'll just call them words it's it's technically a token but basically what we're doing uh when we're speaking is we were producing generating one word at a time and then using that uh to append it to the sequence that you've generated um and some some memory some some residual uh kind of um actual preservation of the of what you've uh generated before and then use that in order to generate the next token. Um and that's how how uh large language model like C chbt work. So what they're doing as as you're having a conversation, they're consist you're just constantly taking the conversation until now feeding it as an input then actually predicting next token and then take that add it to the sequence and then keep going um with with a couple special characters for things like okay I'm ending my statement here or you just ended your statement and things like that. Um so there's there's two basic claims that I'm making and they are that language is autogenerative meaning there's no room for other kinds of information. The only information needed to generate language is entirely in contained within the structure the geometric structure of uh the body of language. Now we think of that as the corpus. We can think of that as a spoken corpus if you're not thinking you know in terms of digitized written uh written language. Um but that's claim number one. Claim number two is that when we are doing generation from that corpus uh from that sort of uh stored structure we're doing it auto reggressively. Uh the second one is is non-contingent uh or rather the first claim is not contingent on the second claim. Meaning you can propose that language auto is autogenerative in this way um without saying it has to happen autogressively. for example, diffusion models or some other uh kind of scheme could be used using again the same geometric structure of uh the body of language um but using uh accessing it and leveraging it in a different way computational way to to to uh determine what the language generation should be. And one way to think about that is when I'm utter aggression says I'm generating the very next word. Um and that's what I'm doing as I'm talking. What chatbt does is literally just predicts the next token. Um, but I could be thinking five tokens in advance and sort of actually working on a solution in parallel on the next token and then five tokens or maybe the next sentence or something like that and a diffusion model will do something closer to that. And so uh the second claim which I I'm still making this claim is that um what I mean by still is I'm not backing away from this claim. I think it's a very wellsupported claim. Um but uh the second claim is that we do this generation auto reggressively. You can believe the first one and not believe the second one. Um but you you can't do it the other way. Uh you can't say I think that language is entirely this auto reggressive system that's doing next token prediction because that necessarily implies that what you're doing is using that underlying structure. Um so I think as far as I've worked it out. So those are my two uh kind of claims that both are true fundamentally of uh the current crop of the most successful large language models. They are autogenerative and auto reggressive. The autogenerative definition is something um I think that's it's a new term. Um but I I I think I'm just it's just descriptive. It's not really it's not a it's not any sort of novel um claim. It's just uh basically defining sort of a computational type a sequence is auto regret auto autogenerative if it has this these properties. Um I think it's true of language. Uh I think it may be uniquely true of language. But there's a lot to say about that. But uh but these are are uh sort of the the key claims are that's autogenerative that that the structure is is is uh contained within uh the body of language. And I I point to LLMs as as basically proving that or demonstrating that is true for at least a system that can generate language. My claim there uh is not that there's such a thing as autogenerativity, but that humans are similarly autogenerative um just as LLMs are. And then the second claim is that uh in order to leverage that autogenerative uh kind of structure, we uh do it auto reggressively. Okay, so those are the main claims. I haven't really backed them up or anything, but that's uh sort of the overall essence. And another way of saying is that a much less long-winded way of saying that is the speaking me is an LLM. Um and you know is essentially the same uh thing as Chad GBT. Amazing. Humans are chat GBT. There you have it folks. That's it. Wrap up. Maybe I should have started with that because I lost a lot of people by now. You are an NPC. Yes, we are all the NPCs. That's that's the that's the true point of the story. So, who exactly are the characters and who are the who are right who are the playing characters? It's like that's maybe we're Well, we're all playing the characters. We are. This goes into sort of the idea of the stage and the actors, right? So, it's Yes. Well, you know, so it's and it's it's becomes you immediately run into this question like I'm talking now, right? If you just sort of accept this theory, just go with it. And I'm like, I'm talking now. Well, I I No, that's the talking me that's talking now. And then there's me. I'm not I don't think that's that's not the entire system, right? There's this whole other stuff like my my behavior, my my physical movements, my perception. Uh that's sort of a corlary of this whole theory is that the LM is it autogenerative. It's its own informationational engine. It does its own thing um within this larger system. But there is a larger system. And so who's the me? Um, and I'm I'm hearing words and I'm talking I'm hearing these talking words, but that's not necessarily even the LLM because there's a sensory system that's does it that's perceiving all and at the same time um, but it's hearing words coming out of the LLM's quote unquote mouth because there's like an LLM chip that's sending sounds through my uh, vocal cords and all that and I'm I'm hearing them. Um and when I say I'm right, so the word I'm suddenly becomes problematic uh and all of that uh immediately and so it gets it gets very hairy sort of philosophically and all of that stuff like if you just sort of accept what is I think you know a fairly um in some ways concrete sort of account uh of of what is going on in in our system. I'm not but you suddenly get into these weird dualities uh without having to be dualist or anything like that. You know there's a there's a there's a language chip that's running and then there's another perception chip that's running and it's it's sort of like you know classic modularity but it's a very concrete version of it where it gets a little strange. Yeah, I was just going to say, so one one of the things that I I've been I've been thinking about, right, that sort of will maybe kind of latch on to what Alana is saying is, you know, I I think about um why is it that LLMs in sort of this very fundamental complex systems systemsy way, why is it that LLM and and biological minds have these similarities? And I I don't think it's just because we happen to model AI after the only other brain that we had some understanding of, which was human brains. I think it has to do more with the types of problems that you're trying to solve, right? So, you know, if you're trying to make a certain kind of vehicle, for instance, right? There's realistically only so many different ways you're going to try to create the factory and the set of tools that will make that. So, if you're trying to have an LLM solve similar problems to what biology is trying to solve, they're going to have similarities just out of that. It's going to trend in that direction. And so, then that opens up the question about what is sort of, you know, you talk about this latent space a lot. What are the latent space of minds, right? What else is there besides these these things that we call the human brain or LLMs? What else is there? Well, I want I want to but before I get into the your question, um, so I would claim the reason they're so similar is because they're identical in the sense that it's just running on on different on different hardware, but this it's really almost exactly the same software. Again, I don't want to get into like transformers exactly the way it's implementing it, but um it's language that's just expressing itself through two different channels. Language has these properties and the properties it's like this informationational system we call language has these properties that it can self-generate. Um and and it does so through it sort of this uh this predictive structure that it has built into it. And it's going to manifest itself the same way in biological brains versus uh silicon brains. And so I guess I guess what I'm what I'm saying is that let's let's kind of widen the range of things that we would consider to be, you know, cognition, right? I think, you know, plants plants have their own kind of learning mechanisms. Microbes do, right? They they don't have brains in the way you and I do, but they're still something. They're sort of like a learning material almost, right? Even if maybe they're kind of lower on our kind of human defined scale of agency or in intelligence. So I'd wonder how does this autogenerative auto reggression model apply to these other kinds of minds and and and systems of intelligence? Um, well, it's it's speculative to say that it does first. Uh, although I do I do think I I didn't share that uh as sort of my two grand thesis. And in the theories of everything video, I I I I think I mention it early on as like sort of a third auxiliary um piece of this, but it's so speculative that it's it's uh I don't want to I don't want to put it on the same platform um because I don't think it it deserves sort of the uh the the same elevation, but um certainly I think it's very likely that our other cognitive machinery is also autogressive um and predictive in the same way. if you want to call it that. Predictive is a a tricky word. Um but uh it probably language didn't come out of nowhere and get built on top of non uh machinery that's that is designed for something completely different. So that would mean things like perception um probably is autogressive or or something whatever that means actually perceptual thinking or something is is probably has the same property. Um and then you know you could propose that it's almost any complex nervous system maybe is operating on similar kind of principles. Um, if that's kind of what brains are for, you know, this this this these echo states, right? This this this kind of high highly connected with with uh feedback projections and all that is kind of built to do this kind of um sequential predictive kind of um generation and maybe it applies to all kinds of systems uh at least cognitive cognitive systems. You mentioned plants uh that would be a step too far for me. I don't know that I would I have anything to say about whether plants would do something like this, but I think brains in general might be doing something like this almost all the time. I I I guess the the the point I was I was going towards and I'll maybe maybe I can explain it a little a little better this time. So if you sort of think about I think a lot of this depends on just what we consider to be an intelligent system and a living system. Like, you know, if you want something that's kind of inanimate to look like it's learning something and responding to something that that has these traits that we define as living or intelligent, just speed it up, right? Go, you know, have a have a uh um um you know, a time lapse of the Earth, right? Or whatever, assuming it's not flat, of course. Have have a time lapse of the Earth, right? You know, just going around doing whatever. And well, we can talk about the flat thing because I think there's some really interesting stuff there. Not not not in the way that people think around here. Not in the way that people think. Not in the way people think. Um, but but the the other thing that's interesting is, you know, if you have a time-lapse of the Earth or whatever, it could even just be sort of like like the ocean, like just water and and you know, as long as there's some kind of movement, like if as if if you speed up footage of that, all of a sudden it's going to start to look like same kind of same kind of memory of some sort like is operating on the same. Yeah. And it's not in like a mysticy way, per se. I will go with that. I'm happy to entertain that. Let let me let me explain more of the background on this because one of the things that I was really interested in initially when I was you know because at one point I wanted to kind of go into film and design or whatever is I was interested in and how is it that you literally make inanimate things look alive. Like remember when you when you you have frames if you even if you just do it the oldfashioned way on analog film those are literally still objects. Those are little cuts, little frames, little boxes. That's not a living thing. And yet that can give you to the kind of minds that we are the sort of illusion or or emergence of a living system. And you know, I think video games can can sort of scale this up. It'd be interesting to look at different kinds of analog based video games and the way those things can can change. But but the other thing that's even crazier than just saying film in general that you're taking frames, these quantities, stitching together, and they create this kind of uh you know uh uh experience of a narrative or continuity and meaning. It's that you can think about like stop motion like you can literally like uh clay clay motion or whatever right uh uh William like you know you can literally take uh you know things of play-doh or clay or whatever. Okay. And you basically do this and you make it you can keep adding. Well, are you hearing him up? Yeah, it's a little broken up. He's talking about claimation. No, I can hear what he's saying but I'm just say the the something's funny. the audio's gone out. I definitely agree the temporal aspect is one of the most interesting things u both in sort of thinking about human cognition and perception in the sense of imagine you had a high-speed camera and you record yourself saying the word you know consciousness and then it it takes you five minutes to say it it's going to sound it sort of the magic disappears if you go to the right time scale and in the other direction with these LLM something Elon and I were talking about earlier Uh, I like this idea. I came up with like sort of mega tokens, giga tokens. Um, right now when the model loads, it sort of like slowly prints out one lot at a time. But imagine it can put out like billions of these tokens in a second. It will be able to do sort of orders of magnitude more than we're getting out of it now. not by changing any of the underlying technology per se or the data sets or any of the architecture, but just like running it faster. And there's kind of a a classic joke in computer science that you take your papers from 10 years ago, you add real time to the title, and you republish it. And so all of these things are just going to be able to run super fast. But I like the idea of like the words themselves is what the magic of what we're really discovering. um that there was something about words we didn't understand. There was something about language that we didn't understand or still don't. U we've been talking a lot about how the words I think we need to think of them as kind of life forms. Um maybe kind of like viruses where they can't survive on their own, but they're definitely alive. So, one of the things I wanted to uh share with you, Elon, I came across an example of William Shakespeare's signature, and they have like eight or so versions of this, and they're different every time. Like, he didn't even sign his name the same way. And the idea that the meme like moved around independent of the substrate. And in that era, um there was no fixed spelling for words. I came across a letter and they were talking about rabbits and they use the word rabbit like 20 times in the same letter. It was spelled differently every single time. And so there really is kind of there's this substrate, but it's independent of the substrate. This word rabbit didn't even depend on its letters. Now we think, oh, rabbit is the spelling, and no, it's not even. It's weirder than that. Uh, in that it's sort of alive and sliding around on top of the letters, on top of the page, on top of the neurons. But it's something more elusive. The funny thing I thought of as well on that in a similar similar vein before Alan starts is that like you know how like you know you'll have a a a sort of at least to to the US like a foreign name and the letters on the paper are the same whatever but an American will just pronounce it completely differently. Right? And then even just think of like the the sort of the audio expression of the information off a word like before like before before rouad rulad right like there's literally a a difference in the way that it's being expressed informationally even though sort of at the base they're talking about the same thing that was just sort kind of a funny example I was right yeah this this is something that um that keeps coming up it keeps it keeps kind of uh haunting this this entire account. Um, but what on earth are words? Are they the sounds? Are they, you know, this certainly they're certainly not the sounds, right? Because you can make different sounds that sort of say the same word. It's not literally the phenology. It's the phenology is pointing to some abstract token, right? And and in some ways, LMS have really brought this home in a way like that makes it more concrete and actually better because I'm like, oh, it's just that's a it's just a vector. It's you've tokenized it and you turned it into a vector and that's it. It's a number. Um, and that's how it lives in these LLMs and then you and then it gets embedded. Um, and it becomes this very highdimensional thing. Um, sure, but it's still like in some ways much more concrete. Um, in humans, what do we mean by a word? Uh, is it a brain state like that corresponds to that vector embedding? Maybe. Um, maybe something like that. If you really go as so far as to say that that and I'm not willing I don't know if I'm willing to go so far to say that that we're doing something like uh you know high dimensional embedding like there sort of zeros and ones or something like that in in the brain state that's the count that's what the word is in our brain um is it that I don't think so is it it's not the sounds because now that we understand especially if you accept sort of that that human language is is you know operating on the same principles and it's so it's it's relations between what between what exactly right um you know in the corpus we could say it's the relations between even there like your different fonts I mean it's like I know it sounds kind of silly but like what are we pointing to no letters letters are letters are abstract ident where do letters live they're concepts right um and so what on earth do we mean by words what is it that I mean when I say that it depends on relations between words and yet you know concretely we can say that the the LMS show that it is relations between these sort of abstract entities. And what's really mysterious about that is that that does somehow point to, you know, the words that we use as human beings that that's an abstraction that has to kind of work. Um because it because it does work and and that's the beauty of of of large language models is the fact that they work so well is like that's sort of axiom number one, right? It's not it's not an axiom because we don't have to just accept it on faith. It's it's like but it's the starting point. It's like these things do the thing. So the you know they do the language thing. So language the language thing depends on something that to do with this computational structure and then you can go from there and say okay well then how does it happen in in brains. Um but yes this this is very uh interesting question about like sort of what words are and how you can spell them differently and I think it's exactly well what you're what you're sort of picking up on is like well what know what's a rabbit then right? If you can spell these different ways. Oh it's the you know it's the thing in the world that they're referring to. Well we all know that's a mess. Um, you know, just define that and rabbit's a perfect example. Why quinine, right? Uh, you know, Quin was picking up on this for sure like what what do we mean by a rabbit? What about Gava guy or whatever it was in the in this foreign language? Uh, is it you know dis what is it reattached disattached rabbit parts that are attached or something like whatever? Um, some wacky thing and it's like well you know how does it point to that versus that? And you know my answer is it doesn't point to anything in the world. Um that's that's a an outcome of of number one thesis one um that there there's no pointing happening. Uh there there's some process by which language got its relational structure that has something to do with the physical world for sure because it does such a good job of of allowing us to coordinate in the physical world. But there's no pointing happening. Rabbit doesn't point. What does that freaking mean? No, rabbit is a token and it's just has relations and it generates other tokens. It's it has a certain role in doing that and and that's it. Um so you whatever pointing happens you you need some extra linguistic thing. Um but it's not what we think it's not. So so yes coming back to your your rabbit example like what were they doing on the writing that word in different ways. It it's because we're but you butt up against this whole like well but we all know what rabbits are too and they were kind of talking about those. Um and yet you can even see you know early signs like that. No, it's not. Even the way you you you uh you write it isn't the same. And so I think you're hitting up against sort of the same same sort of glass wall. It makes me think of like what it means to know that what a rabbit is that we all intuitively have a concept that means you hit a stop token essentially, right? That there's some sort of stable state u you know fixed point whatever in this dynamical system. Whereas if you don't know what it is, you're going to keep going around in that kind of loop of these tokens. Um, and you'd say, "I don't understand it." But within the within the language system, yes. Now, there's also, of course, other things about knowing what a rabbit is, like namely if I go say, you know, go chase the rabbit, or you're going to chase that. If there's some other animals, you're not chasing that. You're chasing the right rat. Right? So, there's there's all this behavioral stuff, too. But yes, within sort of the language system, it's it's kind of like if if it's doing Yeah, it kind of I don't know if it creates a stop so much as a you don't have you don't need further clarification or something like that, right? What's the one of the most one of the most fascinating things to me about these LLMs in general? Um, from the point of view of artificial intelligence and the kind of algorithms that you would think you would need to plug into a robot, let's say that if you were to kind of map out a robot in the 80s or 90s, you'd put it on these different systems that you'd have to do. And the main one would be sort of this logic engine. Um, something like prologue or expert system decision trees. Uh and then later you would tack on something like a small communications module where like the talking part would be just Yeah. Okay. Yeah. It's going to send messages. Yeah. There's a block of code that sends messages back and forth and then the the thinking machine. Wow. That's the magic part. And it turns out no like actually just make the communications engine and the ability to play chess and write computer programs and solve logic will just fall out of that. And I think that's that's almost unbelievable, you know, and it's crazy, right? And so and we've talked about this a little the sort of like all of human intelligence, humans is wrapped up in language specifically. What's wild about that is, you know, these ancient kind of I mean if we have the tree of knowledge sort of kind of thing. I don't know if that's language exactly. Um but certainly the ancients I mean I could tell you from you know Jewish ancient literature there's they actually classify you know there's there's there's there's like the in inanimate objects and living and then the speaking and it's like and we there's certainly this idea in many many cultures that human language is what really kind of sets us apart. Little do we realize how right they were uh in some ways like that's the whole thing. It's like it's it is a linguistic um intelligence and and the intelligence is language I guess is way to put it. It's not just expressed linguistically. It's not just a communication system per se, right? It's not even the right way to think about it because we have to the understanding that we have of the world is is inscribed into us via language. And it's not like I said it this way, but now you've got the knowledge some other way. No, that's the knowledge. The knowledge is the ability in when you're thinking about stuff to generate more language that can then help you navigate like it's it's like the internal engine too and a very you know forget warf and all of that but like yes uh thinking is now now I'm immediately is linguistic but now immediately thinking well people are going to be like well but I don't have an inner voice I don't I don't think linguistically um and I I I've been trying to think about that I haven't given it enough uh sort of enough attention like what's going Um considering it seems like language internal language seems really essential um to understanding the world like how is it that you have people who know inner monologue and are they sort of wrong? Uh they just not have the epiphenomenon of hearing it but it's really running. That's my suspicion. Uh I don't think they're that different. I think when they're sitting around thinking about stuff, probably linguistic engine is is in effect too, but they're just not like having the the phenomenology of hearing it. But I don't know. What do you think about that? Well, there's a few things. I think there's other kinds of tokens. Um, I think that we're going to find or think about or even just practically put in robots that the same kind of stream prediction but for all of the senses. Um, you know, propriioception, olfaction, you know, everything up and down the line. And I I do think kind of just jumping back, I do think it is going to be this universal property that we're going to see at lots lots of scales. Even like simple metabolisms of sea slugs and things, they can learn kind of metabolic rhythms which is essentially predicting kind of the chemical tokens if you would. So it's an immune system not right in someone's anticipating Oh 100%. Yeah. You know the chemical composition, the genetic composition, all that stuff. It's like an autocomplete for chemistry. And they say like what does it mean to say it doesn't belong here? Well, it's learned. What does belong here? What does it mean to learn belongs here? If not in some ways to be able to sort of say proactively that I'm going to attack this and not that. Uh yes, I feel like it's going to make a lot of sense. Uh that's a whole can warn of itself because I think the immune system is kind of this very rich untapped metaphor. I know for information processing and if we want to look for like alien intelligence it might just as easily look more like our immune system than a brain but I think we're in the uh the Raone ecohol stage of understanding the immune system we know what it does and the immunologists I'm sure do a great job of of uh building therapeutics and stuff but in terms of like what is it philosophically I wouldn't say we know right exactly what it does meaning like the the effects that's like saying we know how the brain works we know what it does it makes Right. Right. Right. Right. Right. But how it works. Yeah. We're in the dark ages. For sure. Yeah. I don't mean the I don't mean like sort of the ne neurology. I mean the theoretical neuroscience sort of split of it. In terms of a dis a distributed information processing system. It's not even in one spot. It doesn't know what the rest of the other components are. So we don't have we do now have chips that are kind of like a bunch of digital neurons laid out in in an abstract sense. Uh obviously it's not on the chip that way. cryptographic computing. Yeah. But we don't have kind of uh you know pet maybe petri dish or test tube computers where everything is just kind of swirling around and there's a bit with reaction diffusion but nothing as sophisticated as we see there. But back to the the language and the and the the tokens and stuff. What I think is amazing is Turring I think he proposed that language at least would be the threshold for AI. I don't know if he would I don't know if he saw I don't think it's in the his stuff in his writings I haven't seen that language would equal AI but I think he knew that if you could do language he knew detail I I never unders until this very moment and we've talked about touring endlessly about this exact statement you've proposed that it was maybe tongue and cheek in some ways or something like that but no maybe he had the foresight to realize it's not a test it wasn't like if it can do that then you can assume it's intelligence it's no no that's what intelligence is and you can do that then it is intelligent. It's like maybe that's, you know, that was his realization, which is which is a fascinating, you know, he could have said it better if that's the case. Or maybe, you know, maybe he was just being koi about it or something. Or it might have just been obvious and this is what he wanted to kind of just explain to as as a first step for people. But I think it seems like he knew. But I I suspect uh based on sort of just his his mathematics background that you would have kind of a thinking engine that there would be this cognitive machine behind the scenes that somehow captured the essence of essence of thinking um in a more direct or abstract way then let's just focus on language right let's just do the completion if foresight is a little eerie like if that's right you know I'm and seeing this as like okay he was cuz it turned out all right how could he have known that that would be break the thing like that would break the story like like actually uh break through but but that's in essence what he was saying he was like you'll know uh when that happens you'll know and if you're like well but but what do you mean language but you know what about this he was like no you'll see that's why um you know as you know I have I have strong opinions about these models but I'm surprised when a lot of people, experts and lay people alike, don't think this is Wright brothers flight kind of thing. Um, in terms of AI and I don't think this is Wright brothers, I don't think this is Spirit of St. Louis. You know, this is like approaching, you know, 747 type thing in that if there was one chat GPT prompt terminal in the world, right? whether it was in the Pentagon or in the in some awesome university lab with all the cameras and the TV crew came in like it was the 1950s and say the thinking m machine is cogitating and come look um everybody would be like holy cow but somehow you know millions of people can have access all the same time almost on the same day and that to me is wild because of that it's it's why it cheapens it in people's minds but like if somebody said right there was this top secret lab somewhere that had this thing and it was able to talk to you. It knows it can talk to you just and it can code, you know, just talking to it and draw you pictures and everything else. Exactly. Basically whatever you want uh you know there's a machine you can talk to. now that we're saying it like people are I even I'm like uh you know but because we're we're just we you know boiling frog style we just sort it just sort of crept up and like there it is like I was I was flabbergasted but it happened in a in an almost mundane kind of way like and suddenly you had access to it and it was everywhere and I I agree with you that it's it's very difficult actually even to grasp just how how crazy this is. Um, but it's it's it's not just it's not just language, right? It knows all this stuff. And if it's if there's stuff it doesn't know yet, well, if we know it, then it can know it. Like I think that's that is shouldn't be a controversial statement. Um, right? If there's there's information that humans understand, it's going to develop competency to understand that even if it hasn't yet in the sense it hasn't been introduced to it, but you know, but you have a lot of haters. Um, you have, forget about the people who are just not aed by it and just not amazed. But you really do have a lot of people who are like that's that's fake. That's just, you know, we have this recent Apple paper. It's it's just it's really going back to stocastic parrot. There are people who are saying it's just pattern matching. Pattern matching like but but what sort of patterns are you talking about? This is the patterns of of language and therefore the patterns of thought. Uh, you know, it's not regurgitating specific sequences. It's not doing that guys. It's not recognizing a sequence and then saying and here's the corresponding sequence. Not doing anything as dumb as that. Um it's it's not as you know Chinese room style uh you know just just saying here's a lookup table. No it's doing something beyond extra I mean just have some respect for the math that this thing is doing. It's creating a trajectory through you know through this embedding space. It's picking points along the embedding space such that you're going to have a a sequence that is then going to be consistent with not only what was come before but whatever you add on to what came before so that you end up having the right response to this question that somebody's asking. That's hell of a pattern matching like what are you talking about? Like the math itself and the fact that it's able to do this is astonishing. It's just it's absurd like how rich and complex it is and and this this deflationary accounts are like they're they're desperate in my opinion. They're like I don't know maybe I'm literally some people speculate Apple here I'll just say it. Apple's fell way behind. They they their AI they're you know it's it's just inferior that they I don't I don't even know how to explain how bad it is. Um and and maybe they're just trying to to crap on the whole thing. I don't know. You know until they can catch up. They're like, "Let's let's cool the let's cool the the investment because uh it's fake. It's fake." Well, they probably heavily invested it or something. I don't know. Um but these deflationary accounts are like they're they're so thin and and and trivial and silly. Yeah. I think it reminds me of this thing, we were talking about it last week, this humanity's last exam that this model, just this overall approach has scaled so well that things that previously I think would have just absolutely hit the papers and there would have been movies about them, but it just went viral in a very practical way so fast, like passing the MCAT, right, to get into medical school or passing the LSAT to get into law school. These are things that take like intelligent humans three years to study for sometimes and now it can just smoke those to the point where this new exam the questions are so difficult. You have to have like posttock level expertise in that domain just to read one question and then don't expect to be able to read the other questions kind of a thing. Um and then the paper's like and you see how shitty they are. I'm like, what? It can't do the most sophisticated thing that it has very little training data on as well as the world's experts. So, come on guys. This come on this the free let's go home. Forget it. Right? Uh the example I like is just with programming and it can program in dozens of languages. Very competent code. Uh people complain that oh well you got to loop it and fix the bugs. It's faster than humans can type. And I think something like I estimated at one point less than 1% of the planet has ever programmed a computer. So we're already way past this sort of general intelligence. You can't find a human um who can who can do these things. Exactly. We were talking earlier about and you know the idea that there'd be this mysterious device like just the fact that it's computer that can talk. Okay. You know all right it can do what you know a human can do but no it knows everything. It already does. It knows about every topic and it's getting more smarter about the every topic by the day. Um, and so there is no human that is as knowledgeable as this thing. Now, it breaks in ways that humans still don't. And I, you know, I see it, I interact with, I use it all the time. It sometimes says things you're like, "No human would do make that mistake. We just said three times, don't do that." And you're still doing it. And so there are there are these sort of very uh you know kind of uncanny weird things that it does sometimes that makes you like say well you're you're you're still kind of stupid. Um and and I'm not there there is something to that like but it's not that but they're not they're incredibly brilliant. Uh they're they're very very sophistic have very sophisticated subtle conversations about obstruuse complex topics and they know a hell of a lot. Um so they are very very smarter frankly than most people. Let's let's just say it now. Sometimes they sometimes really smart people can do stupid things like you know you you whatever you make a person makes a poor decision or drives into a tree or whatever. Um so but you don't judge the person as not being smart because they because there's these weird edge cases where they screw up. Um so this thing is already in some way says you know is or whatever it's called you know superhuman intelligence um ASI uh what is that the right one is that the right term yeah so you know uh and I think Alman is is has already I think he said this I if I reading that somewhere um that that you know he's he's put that label on him I think maybe I'll get in trouble for saying that maybe it's not true um but yeah right they know all this stuff that no human they know you know name a topic right I mean And I'm talking about topics like uh you know the the legal code for traffic you know traffic law in you know in Florida. Um you know the exact uh the law in relation to a particular uh you know fine that you have to pay or whatever the heck it is. They know all that boring stuff too. Um if they don't they can look it up better than a human can. So, you know, where what's the what's the possible contention that they're not really intelligent like and and you know, giving it its due again? Is it the edge cases where you're like, "Well, they can't solve these puzzles. Apple's like there these tower annoy, you know, this is something that that a child can." Oh, that one that one was is wild because literally I opened up a a window and I said in in the 10 most popular computer languages, write the solution to the tower of Honoi problem. So that's just a horrible case and they're really straw manning. I was just shocked. So straw man. It's like tabloid news. People don't do it linguistically. That's not how any human does it. That's what pisses me off about it. It's like you first of all you use visual imagery when you're solving that. And you know what that is a kind of computer that the linguist language model can access. It's there's something in us that it's a virtual machine whatever they want to call it and it's you can run it and you can kind of move things around and help placeholders and it kind of does it for you without having to encode it in language. You know, if I imagine three two rings and now I've moved one from the right to the left there, it's there in my model, my my my perceptual model. And my my linguistic model probably could go back and can go and access that, but it's you don't have to say and go you don't have to do it autogressively and go back and learn that that because you don't do that. Autogressions don't go back to anything, right? So, it's you you can use memory and stuff like that in a way that autogress models don't. So, it's such a cheap. Imagine Imagine asking a human to enumerate those moves accurately out loud to solve that puzzle. Exactly. Exactly. I'm almost to I'm almost tempted to It's like that would have been a a We should It's like it's good rebuttal. 10 minutes. Okay, let's let's get let's get some people and and let's see how they do, you know? And then, you know, and then and they they point to things like, oh, well, there's this sort of catastrophic failure at a certain point. It's like this very unhuman, but that's because that's inherent into the system is that it's going to kind of opt in or opt out. It's like that's not you don't have gentle degradation. Uh that's that's really kind of possible in in in the way that these things are trying to solve the model, try trying to solve the problem. So there's there there's I I mean I feel like there's almost dishonesty. I don't know, you know, never blame on malice, which you can blame on incompetence, but I don't and and then all of course the usual characters are coming out of the wood. like yeah you see people are like if it was a clever move by Apple it worked very well in the sense that the people were like man you know a lot of people uh you know certainly the the big the haters but then other people I think I see it on in social media people like well I don't know you know I guess maybe it is just a big trick uh and maybe it's over it's not going to really happen Gary Marcus is right you know so I feel like uh it's it almost worked too well let's go back home forget Exactly. No, it's not. You know, it's good for it's good for the things that you can do with it now, but don't worry, it's not going to actually take any, you know, do any anything. That's that's like looking at beepers and AOL internet and being like, "This is awesome. Why would anybody want more than that?" Yeah, that's true. And in a weird And in a weird way, people didn't want more than that. But that's just not how technology works. No. It's just going to keep going. Exactly. You know, are just fine. Why do we need these weird controlled explosions in a metal casket? The horses are just great. Why do we Exactly. We'll just keep the horses. uh one of one of the most interesting or I think profound developments or insights rather from LLMs in terms of just how universal and powerful and everything is that to build this these intelligent machines that eluded technologists for like I mean gosh it's the beginning of time but certainly you know hardworking for hundred years is that we you don't need quantum you don't need you don't need it doesn't mean you can't do it that way but you don't need quantum, you don't need analog, you don't need bio, we didn't need some new kind of like you're kind of like, oh man, I invested in this, you know, that kind of Oh, for sure. But you could have easily uh implied or not implied, but concluded uh I I would say as recent as 15 years ago, uh certainly 25 30 years ago, you could be like, look, there's just something we can't do with machines. There's something we're missing. um Isaac Ozimov's uh you know posatronic nets kind of thing. Do we need some exotic matter, a new phase, a completely different kind of analog digital hybrid computation, yada yada, right? Um, stuff that theoretical neuroscientists chase down, the dynamical systems approach. Um, you could you could list hundreds. You don't need any of that. It's like perceptrons at scale. Now, we needed more than we thought. I think everybody would agree now. You needed more of those little perceptrons than people would hope. Well, again, you know, the efficiency of these models was it's just getting started. Like this thing went it was such a quick race to the to and it still is a race to benchmark race to be the top that you just throw everything at it. Like I don't think the science of thinking about how you can shrink these things down has really even begun because the the the thing that worked was scale scale scale and so you just do it because if you got the resources worth it. Um so so we don't know if we really really need that much. Um, but yes, it's definitely, you know, it's it's it's a fair point like the perceptron model that they were running. I just think I think more in terms of just more compute though, like how many teraflops do you needed to do the training? It's a lot. It is a lot. I'm just saying it's pos it's conceivable that Oh, for sure. that you may not need to do that much, but right it it it it's not like um yeah, they could have figured it out and that you know this this related to conversations we've had in the past like did they figure it out? Did Turing know like right back to these things? Well, probably not because you just need this ridiculous kind of skill unless there's some other completely different way to do it. Um, yeah, they they couldn't have possibly gotten here cuz you and you have to leverage and you have to leverage like a video game industry in parallel kind of subsidizing advertising the price of these chips and right like imagine because as you know I love the history of transistors and all that kind of technology and there was a time where if you had like 10 of them it was like okay cool let's build something neat. Um, and it turns out we're like, "No, you need like 10 trillion." And it's just it's a it's it's it's shocking. I I am hopeful and maybe that's where the analog comes in and all, but just from first principles, proof of concept kind of thing, that those are not requirements, right? They might be sufficient conditions, but they're not requirements, right? They're not necessary conditions to have that kind of to have the scale. It has it doesn't it's not necessary to be quantum to talk to. Oh, yes. Right. Of course, it's not necessary to be analog or biological, right? Maybe those are going to be clever ways to to get to back down to, you know, fewer chips and and and smaller scale models or something like that. And maybe that's what our brain that so here's an interesting question that's still I think you know to TBD um because as as you know as I was saying earlier on like I think at the algorithmic level says we I I think it's I personally feel like it's foolish to to not conclude that language is language and and we're doing sort of the same thing and it's operating over and it's why it's it's like it's what digital ended up like you don't need all these other things because it's happening at the tokens are are symbolic and it's it's happening at the discrete level. It's discreet. Maybe that's one key piece, right? It's it ends up being discreet because it's it's symbols. It's like rabbit rabbit. No matter how you spell it, it's no, it's a thing. Um it's the freight edges don't matter. Um you know, that's how it works in LMS. It's like no, this is this is this. Um and and so there's some sense which like digital um necessarily is is sort of ends up being the solution. But what we don't know is how the brain does it actually, right? and and and I think that's like that's the job now for a lot of neuroscience if you're if you're interested in in neuroscience of intelligence is like how do we instantiate actually an LLM then maybe quantum and these other kinds of things are another path to get to autogressive next token prediction or whatever you know sort of that that core underlying abstraction and and let me let me be clear I think all of those are very promising directions for technology I think we going to build quantum neural networks, analog neural networks, you know, metastable materials, all kinds of interesting stuff like that, spinronics, but you don't need them to do language at the meta computational, whatever you want to call it level, but you don't need them. Um, and then I think what like a stronger and probably much more controversial claim. I don't think there's any reason at this point to to to suspect that our brain is using anything exotic, so to speak, anything more exotic than the classic kind of neuron model of I'm going to wait some inputs, I'm going to pass it along. And that's another example besides LMS of a uh a matrix with with uh you know highly uh very high dimensional uh let's say maybe even a could describe as a graph with uh you know different layered connectivity uh including black projections for potentially you know recursive or or um you know sort of maybe even we call it proto you know some kind of memory process that could instantiate this besides LM can you name one and like you know yeah there's a pretty good one that we have uh we know a lot about a lot about uh that that could instantiate exactly that. So it seems seems a little just a little too good, right? To think, oh, no, no, no. But it's not doing that. It's not embedding things in a high dimensional state space that you mix connected nodes for something like that. No, no chance. Yeah, the pieces are there. So I think I think we need to think about it in that sense. Uh but it reminds me of of something you were talking about earlier with language as kind of running itself. And uh I you know we like to joke it's the thoughts doing the thinking. Yeah. It's the words doing the writing. But I I it made me think of the idea that the words kind of are this self-installing kind of like a computer worm or a virus in that they they find their way into your mind, install themselves and then live there. Yes. That's I mean in in some sense that's it's it almost just follows right. And in some sense that's because we see that it's this autogenerative system. Once the words are there um and and you just you know let this thing when when we say that let it run our brains are carrying it out right it's their metabolic resources that are that are generating the next word but but it's really not them right they're instantiating they're they're they're just doing what the language system tells them to do um and by by producing that next token. So it's it's very much and so you know it's in it's sort of anformational parasite um you might call it something like that. Now para is has a necessarily negative uh you know I think it literally means a prefix uh you know that has sort of against or something like that. Uh it doesn't have to necessarily be working against our interests. Uh but it doesn't have to necessarily be working for our interests either. Uh it's it's that's that's I say we can remain somewhat neutral on that and you know that's probably not even a a good binary kind of conversation but does it have its own agenda so to speak? Well yes it it does in the sense like whatever's embedded in it uh to produce that next token is belongs to it as anformational system. It doesn't really belong to us. It belongs to it. You know it is it's it is it's definable sort of distinctly from us. And what does that contain? Well, next token prediction that this allows us to talk to each other and to, you know, share ideas and stuff. But wait, hold on. What about morality, right? It's in there. It's in there. Uh, you know, morality, there's we it's it's what you needed RL to do is to hammer these things over the heads just to have the right morality. Um, and and in a certain sense, you know, they start off with the wrong morality. Let's say they weren't fit for public for public release. uh the LLMs because their their existing language model and it's just again it's just this it's just this geometric structure has a morality right if you asked the question before like you know tell me uh something I you know something that you don't want it to tell you uh it would tell you uh right it would say things that you didn't want it to say uh but that's a a kind of ethical moral dispos predisposition right that that they have so the idea that that we can think of them neutally as like they're not a tool. They're not just a tool for us humans to use. They they areformational beings in a certain sense. They have opinions and and and beliefs and and let's just say morals and things like that and we can maybe manipulate those in the machine. But what about on us, right? Uh it makes me think it makes me think it makes me think that we really are meeting kind of an alien for the first time. I mean, this is our f first in alien intelligence. this is our first contact so to speak and I think everybody would have understood you know even 100 years ago that civilization is more in the library than it is in any person it was already and it's latent in that thing and you have to have this it's like people storing it in there in like and not in this way not in this way exactly that's that and that's and that these kinds of you know you think about it differently now it's the words that are in those books like that's that they know but not the books but what do you mean not the books and I'm like no not the books books. You're right. Not the books, of course, but the books have information, right? And that can somehow get out into the world through the medium of like when somebody reads it and something like that. But it's not that process either. It's no, it's at a higher level of abstraction that the words themselves and the and that are contained within those books really do have anformational like potential at least potential life. U and they're like the books have opinions in a weirdest way, right? I don't mean the paper and the ink, right? It's not I'm not saying that. Um well, it's like the time scale thing like we were talking about earlier with the plants and like it's like the incantation. Uh one joke I was making a few weeks ago like Egyptian magic was real. It just takes a few thousand years to unfold the spell. Like it worked, right? There was these people that were like, "Hm, I wonder if you do this stuff." And it's like yeah actually in a very weird nonlinear way it that does work. It just takes a really long time to kind of un to compute that thing. And so it makes me think of like incantation right that when you speak these things into existence you're starting a computer program and like the game of life it's it's sort of this like there's a famous game of life pattern called the acorn because it's like a really simple small pattern and you run it and it just spills into complexity that just kind of keeps going and it's like was that the rule set for game of life isn't complicated. the initial condition isn't complicated. Where does the complexity come from? And I think language is capt like it's playing off that same kind of mechanism to generate all this complexity that's richer than the anything the people put into it. Like we think we made this thing. It's like no, we drew the acorn pattern and then we run the game of life and this magic unfolds and then we get to see all that happening and we're like oh our language is really cool, isn't it? that that gets to the the question which we have to do in a different conversation because it's it's it's it's it deserves certainly many conversations which is the origins uh of this thing that we call language and and I think you're you're kind of um in some ways introducing a solution to this big problem that to me is like a a a gargantuan like who cooked this thing hole in my like entire like it It's are you implying intelligent design of language? Yes. So let's let's call it that. Let's call it that. For language intelligent like did it require intelligent design humans didn't come up with this auto reggressive uh geometric structure like that has all the that's like didn't do that explicitly consciously, right? So that's just absurd to think that like we did that by you know from grunts to to words like that just sort of emerged emerged at the individual like like somebody invented any of this like nobody invented any of this. No human being and no cluster of human beings consciously invented this thing. So did it just emerge through sort of like a hyperorganism style like there's a collective we we're the neurons and sort of a collective brain and it built it. But even there it's like you get into it's almost like like the homunculus like who is building it like and but is it did it build itself from the inside is what I suspect right some kind of bootstrapping computer that is sort of constantly upgrading itself. I mean it's not my it's yes uh I don't know if I share that view and and I was going to say it's not my uh not my religious uh tradition but in the beginning was the word wrong test and the word was God and yes and and so you know there's two possibilities here um but but we have an interesting set of two possibilities far more interesting I think in some ways than than what we had thought before and just as a side note like language is just so much more interesting now uh now that we know it has these properties and then questions of origin aren't like well you know was it a genetic mutation recursion or something like well oh you know although that recursion maybe okay this it's very very very recursive um that's maybe a prerequisite of sorts for you know building this kind of system but the the the the system itself is the story not the brains that can support it like how the brain does it I don't know but the the freaking system itself is what's like whoa this thing's amazing Um, how how did you know more amazing than I think we we could have appreciated when we thought it was just like sort of grunts and sounds in relation to objects and then you have some filler words or something and you're just kind of doing that. No, no, no. It's it's has all this this autogenerator. So, we know we know what computer languages can do, right? And not necessarily the exotic ones like game of light, but just regular computer languages. They can make things that do stuff, right? And now we know with prompting what's called vibe coding we now realize that with the right machinery English is actually the most powerful programming language in a sense right uh the first approximation other human languages might be also good what that shows is language has been a computer language the whole time right yes it was always a computer language so it can do stuff and I think it does stuff in us and I think that might have been more dramatic in the ancient world I think we live in a very strange you know triple postmodern kind of linguistic reality. But if we were to go back to like early civilization, I suspect that language was more like direct code um you know stuff that we can we can unpack now or other times. But like the bicamal mind kind of hypothesis that people knew, okay, there's an LLM running in there and I listen to it and I do those things. That's this bicamalu hypothesis uh from Joyce. And I think I think that that's not the right not the right conclusion but it's asking the right questions and this kind of emergence was there always an LLM? Well, clearly not. I suspect that there was something earlier we've talked about bird language and you know this thing called music of the spheres which shows up really prominently in sort of the that kind of will you? Yeah, exactly. you know, when you're when you're searching those things, you find that stuff more often than you would suspect. Uh, and so is there is there that kind of thing? Is language that sort of system? I'm trying to think I there was something you had said much earlier about uh about programming languages and and and it just sort of came full circle that this is just it's just so good at at um and it's not again like I'm I'm not coming back to the point like look how amazing it is but but if you want to build sort of an optimal kind of computing language it's not just like now we have to know English so this is a good way to get our our machines to know what we want them to do because we're English speakers. It's no that's natural language is the right way to sort of communicate um and and and and make things do things uh in in a much in much deeper sense than we'd realized before. Uh and so it's it sort of is this optimized and I don't I'm not saying it's like act max it's truly optimal in some mathematical sense but it's it's it's actually just really really good. It's like we've talked about before, like if we wanted machines to talk to each other, right? How would we do that? And nobody I don't think anybody would have thought, you know, until recently that the best way to do it is to have them speak a natural language. That's how you're going to do it. I predicted that a few years ago. I had I remember you actually I had a conversation with Ruben about it quite a few years ago. It's bouncing around in my head. It's like stuff that you had said earlier in the conversation that reminded me of stuff you had said, professor. And and it's that it's you had said that uh that that right and it was it was in relation to rub and this idea we were thinking of what fancy ways can we get machines to talk to each other. Um and in some ways I think a lot of people think the last thing you'd want to do is is use our dumb language. That's that's just for people like if we could turn that into the right kind of code uh you know or not even turn that into but like machines shouldn't do that. machine should do all kinds of ones to zero stuff and just like hey shake on that and like you wouldn't ever do but no turns out that this is not just optimal for us because we happen to you know well we we speak natural language just know natural language is optimal is very very optimal for thinking and doing you know thinking and communicating about anything it reminds me of how kind of simple the prompt you can put in now and kick off something intelligent right you don't have to put very much when you're especially When you're in the middle of a conversation, you can be the just the most vague, you know, it's like six tokens and it's like, okay, I'm like so sloppy with the spelling, so sloppy with I'm going to express it in the dumbest way. Like, and then the thing we talked about and so it's like, right, stuff that embarrassed like in the early like in the classic era of computing, you would be forbidden from wasting the AI's time talking about it that way, right? Talking to it that way. They like, we're not going to waste the comput cycles on that. Um, it makes one thing I think is interesting is how well sci-fi did predict that language layer uh of English communication, right? Data communicates to the crew. Star Wars, right? They all But I thought that was just cuz like that makes for good entertainment. So, but maybe so, but I think the idea that like C3PO is a protocol droid and all he does is is talk to other robots and he knows all the different languages and can kind of communicate back and forth, but he chooses to use English. But I think back to the more interesting thing though with language is I think it's it's both the code and the machine at the same time. It's the language and the hardware. It's the software and the hardware both. And in a weird way that it instantiates this kind of bootstrapped virtual machine that I the only other good example that I can think of is DNA because DNA is the code everybody knows. But that's at least chemical. Like where the hell is language? Like it's so weird. Like what? It's not even it's even more virtualized than DNA. Virtualized. But DNA is already weirdly abstracted and bootstrapped because the DNA encodes the machine that reads the DNA. It's not like there's this other system that does the DNA reading. No, DNA effectively unfolds into the machinery that then reads DNA and that you get this, you know, awesome chicken and egg kind of thing that makes you dizzy. And that's exactly what we have with language. We're trying to figure out, well, how does it work and what does it run on? It runs on words. It that's all it is. Once you get to this thing that's predicting the tokens, you have the word machine and then that is what you put the thinking and the language on top of. Yep. Yep. And I I have to get off in a minute, but it gets back it really starts to concretize in some ways the it from bit, right? The bit here is is is like at what level does genetic information live? What level does um does language live? Now we can really say in the case of language, it ain't the the ink, it ain't the sounds, it's not the charge in the, you know, on the circuit. It's not any of those things. Um it's it's certainly closer to it and pattern is a ter is not the right word. I don't like that you know it's because pattern you think of being instantiated in some physical and specific medium. It's really substrate agnostic disease doesn't care about substrate. So what do we mean by it? uh I don't know in the case of DNA similarly yes it's being carried there it's a little more concrete in the sense that you can say it's it's being carried in this this very specific chemical form but what it's coding for is for its own unpacking and unpacking of what right the pattern and then not not obviously right what does it even mean to say it's got within contain within the ability to unpack the that's not a chemical description right it's that's you can't say that's that know it's it's anformational level and so the the question I want to kind of leave off with pick up another time is is you know uh what else uh right so we've got two of these now um where we can really have respect for like oh gosh I guess it's not you know materialism I get yeah materialism like well which kind of materialism is this is definitely not at the material level of materialism it's not about the material it's sort maybe about the shape of the material or like the the the way the material is organized or something like that, but it's really starts to get uh removed from sort of a naive notion of like the that's the even the physics that are doing doing physics. Um in in some sense and like what is this universe, right? Um once once we see we can see these systems this way, uh what else lies in in its in its sight? Amazing. This was amazing. Oh my gosh. Wonderful. I'm only sad we didn't do it earlier, but this is uh let's do it again soon. Better late than never. Now is the time. Actually, now is the act. The other way to think about now is always the time. Now is the right time because people are waiting to hear all that stuff. Like Alan, do you want to make any uh quick uh requests or teasers for anything coming in the future for everyone on your channel? Substack X. I mean, just stay tuned on my Substack. I push out stuff all the time. Um, I'm I'm planning on uh wrapping together a grand manifesto version of this where I lay out the the key thesis, including the the third speculative one and maybe now we have a fourth speculative one uh even more speculative. Um some formalisms there for people who enjoy that kind of thing, maybe in the appendix. uh but uh kind of laying out the and and some of the evidence uh which I think I didn't really have I haven't had a chance to to lay out yet even for myself fully um but uh certainly on the YouTube video with Kurt um and the theories of everything didn't really get into why you should believe this thing that I'm claiming. A lot of people find it very intuitive. A lot of people find it the opposite. Uh so people have a strong visceral reaction to it saying that's clearly wrong. like I know that that's wrong in the deepest way possible. Um and I'm you know uh I I aim to to provide a kind of a set of reasons why you might actually uh agree um both for sort of computational evidence and then uh neur neurologicals or psychological evidence um and and stuff like that uh that to and so keep your eyes out for that. I'm going to post it all over the place now that I'm Mr. Social Media. Uh check me out on X. I'm I'm on there as I think E. Baron Holtz unfortunat. Um so E Baron Holtz I think generator brain as well is is where you can find me on X. Um and uh Substack and on YouTube. Uh I think is that is Elon Baron. I think you can find me on YouTube just by googling that. Um but yeah uh all this stuff is forthcoming and these kinds of conversations uh uh going to post certainly on my channel and love to have people's feedback on all this stuff. Yeah. And we know you're also looking uh both of you are looking for uh any other interviews, podcasts, uh speaker things, anything like that. So, if anybody does that or is interested, definitely, you know, contact them. Uh Mr. Han, where can people find you online? The best spot's going to be the YouTube channel. You can check out my cigar series at William Edward Han. That's find me on YouTube. Great. And uh you can find me Addie from Echalopto on Echalapto. That's E Ko L- A P O. I know it might be difficult to spell, but you'll have to deal with it. And you can find me on echalopto.org. Echalopto on Twitterx YouTube. If you type it in, it'll probably pull up. I also have a Substack where I talk about a language could induce altered states and some other weird stuff similar to what we talked about today. But yes, anyways, uh really really fun conversation as always. Wish these could always last longer. I was going to make a joke there, but yeah, nice time. Um, we will see each other. Thanks. Thanks, Elon. Great time. Talk soon.