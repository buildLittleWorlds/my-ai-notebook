URL: https://www.youtube.com/watch?v=TOCL40OSF2E&t=18s
==============================
Channel Title: ekkol√°pto
Video Title: AI Professor Explains The Qualia of INTELLIGENCE and LLMs | Professor Elan Barenholtz PhD
Publish Date: December 12, 2024
==============================

TRANSCRIPT:

okay so what I've been thinking about and this this kind of taken up all my mental SPAC is we are llms and I don't when I say we are LMS I just mean the speaking we the the linguistic we the the thing that's doing the talking right now that's talking to you is is an llm and and what are the implications of that so I I'm not going to uh spend time right now think trying to convince anybody that our language has to be allance but in a nutshell it's just completely implausible that language would have the structure that it does these long-term dependencies that end up working out in an auto regressive um environment it can have that structure and then humans are doing differently that's just that's absurd so language is it's our thing before it was their thing before it was Machin was our thing and it has this structure the structure is designed to generate autor regressive uh next token generation and that means that's what we're doing so we are llms and now the question is what are the implications of that and there's kind of scientific implications that I think are very very important um the brain has has to have some way to encode sequence uh such that it's able to go back in the past and I don't mean literally in some ways that it's it's not I don't think it's a Memory in in the traditional sense it's somehow it's built into the way the the brain encodes information and and the and the um the topology you know it's probably things like just kind of rear feedback loops and things like that but it has to be encoding sequence and that has implications in terms of thinking about what the brain is actually doing what what uh how neurons work and it might not just be about language I don't I don't know yet uh but certainly language the fact humans are doing it in this autoaggressive way which means that they have to be uh preserving this kind of sequence and I think that's going to have very very big implications for Neuroscience um and thinking about how the brain works but then there are other kinds of implications um which are harder to think about but also you know incredibly potentially incredibly profound because things like saying that the thing that's doing the talking now is not the same thing that's doing the seeing now and we can think visually and we can think linguistically um and there's clearly interaction between them but they're the they're they modular systems and because LM speak basically demonstrate that that that the Corpus of language and the structure in the Corpus of language is enough to be able to generate itself without any it it it does it effectively the generation process does not require any sort of um uh interaction with some other uh sort of computational framework like vision for example um llms can talk about red and they can do it very effectively uh they'll tell you what red means and you know scientifically uh they'll tell you what it means to what it feels like uh to to experience a red sunset uh you know try it out uh it'll be very expressive it'll sound like a person who's able to express um ideas that that I think we would somehow anticipate naively would require actually knowing what a sunset looks like and what red means but they can't know what means because they don't have access uh it's simply not in their computational architecture at all the the quality of redness that the the visual system uh in codes isn't in there uh in in certainly in in you know a purely text trained model so that means in us as well the linguistic system that's generating doesn't really know what it's talking about when it talks about red and but but then there's another system in us that does know what red is uh that's the visual system and I I think this is a uh a very important Insight in relation to the Mind Body problem and why there seems to be this du this Duality is because we have these incommensurate incommensurate they're just separate computational systems and they interact they message pass they talk to each other but they don't speak the same computational language and so I think that all of this has has very very very profound uh implications I I could go further but um that's already a lot and so that's what I'm thinking about these these days and I I think I think it might be worthwhile you know for people to think about some of this stuff and it's not not specifically my ideas but I'm a little surprised by the lack of fanfare in some sense from the you know the intellectual academic community in relation to these very profound insights that these llms seem to be affording us uh because these These are you know these These are these this has implications for traditional Fields like Linguistics and and philosophy um I think they're immediately apparent so uh I think this is a an inflection point I think we could say in you know sort of an human intellectual history whatever so I think I think it's reasonable to I think I think we're llms I think the the the speaking us the linguistic us is a large language model and I have lots of reasons to think this and uh uh I could share those with you but I think if we just draw that conclusion and go from there there's some very interesting mind-blowing kind of realizations you have and the thing to know about LM so technically what they are they're these kind of learning models you you dump a bunch of data in this case it's linguistic data sequential linguistic data things digitized books digitized websites but it's all language and you dump that into this machine and it learns language I think those who claim at this point that it's just some sort of mimicry of language uh they either haven't spent enough time with it or they're career depends on denying it or something it just doesn't make sense uh it knows language it knows it understands the concepts linguistically it performs linguistically in such a way that's if not indistinguishable close enough to human re that okay you got to fix some stuff but it it knows language and it knows language based on language alone because as I said it's a machine you feed a bunch of data and it learns about that data in this case the the data is language data it doesn't even have to be language data but in this case it's language data and this kind of machine is training enough can go it learns actually from the data it's not just like input output but it turns through the data it kind of thinks about the data almost you could say and by the time it's done uh it's learned about that data and what it's learned in this case is sufficient for it to be able to produce language in a way that's I think arguably um as good as humans in many cases better than humans uh but the basic concepts are there uh langu it knows English really really well and it's so it's doing this just based on langu linguistic data and so here's the mind-blowing thing the mind-blowing thing is it doesn't know what red means it doesn't know what space is it doesn't know what feeling anything means it doesn't know what sounds are these words that I'm using right now I I'm aware that it's a little bit uh strange to think about the the thing that's talking now uh is is one of these language models but we'll leave that aside for now but they never they're never given that data of what the visual system sees they're not given data about what red looks like the visual system is and the visual system sees red uh because it has a very particular structure also learned to a large extent that processes date in a certain way that that redness has the quality that it does but the language modal never learns about that at all unless maybe it's got some books like Mary um you know the famous Mary problem's read some books about how color is represented and and about the the the photo receptor array and it can maybe kind of contemplate linguistically um what it would see mean to see R but just like Mary uh coming all it's very it's implausible it's there's no reason to think at all that it has any computational idea of what redness is because the visual system has that and yet it's able to talk about red all day long and it knows what red means linguistically it will use it correctly if you tell it in the beginning of a story this is a red ball it will use that information completely appropriately linguistically for the rest of you know the rest of the conversation and so what that means is that the language models are able that the language contains within itself sufficient information to generate itself fully without having any access to what the words are actually mean and when I say what the words mean that's what I naively mean that's that's that's what we mean by Red we mean when we feel like we mean that but the language models don't really mean that so what the hell is going on and how are we able to talk about qualitative aspects of uh you know the the visual environment when that information about uh the the sort of the what the what the words in some ways refer to and they have to refer to it because obviously we can communicate I can say go get that red ball and you know somebody will go get it and I'll get the exact right one there's some obvious sense in there there's a mapping here but language is self-contained and if it's self-contained that means language not only doesn't need the it doesn't need the visual information it can't be in there at all it's it's strictly linguistic strict linguistic is what generates Ling you can't slip it in there in some way and so that means even in human beings even though we have this sort of clear compatibility and exchange our visual system and our language system and and not just by the visual or you know smell sound whatever all the experiential stuff um is something that's not in the language like computationally and so that means that this really is this weird Schism this this weird Duality that I Su you know I I I went into this career my my entire career was was to because of the Mind Body problem I I thought that was the most interesting thing in the world and I I still think it's the most interesting thing in the world but I feel like there's there some actual insight into now what's uh it's it's it's a different way of looking at it what the source of the problem is and I think maybe this is very important that there's this computational in compatibility inherent it's not in it's not the right word but they're mutually exclusive what can I say the linguistic system speaks language and that's it um it doesn't speak vision and vice versa and there are aspects of reality that maybe the language system can sort of capture and whatever that would mean that the visual system may not have access to um and so and I keep focusing on visual but I I mean all the whole sensory domain and by the way all Consciousness is sensory I don't all all conscious sensory Consciousness and so I think that that's the ghost in the machine is is in some way that it's it's there's there's that the the the language thing the the speaking thing is is is cut off from that way of uh interacting with reality you know call call it that whatever that it's a computational system but it is it interacts and then the linguistic system is is um is incapable inherently incapable of uh knowing it and and and therefore the language system is like what the hell is this subjective thing it's a word for it but I don't know what it means and it doesn't fit and it it sort of needs to be expunged because it doesn't actually run sort of in this in in the linguistic system and so that that's I think sort of where the sort of you know it's almost like we can look at the the the Corpus of philosophy and all this as having been generated uh by language models that don't understand what red is and that means uh now we sort of in a weird way explained the Mind Body problem now we can go in in Loops here about uh you know what what is what's the next what is what where does one go from there and and I'm not there yet um but I think this is a a very important insight