URL: https://www.youtube.com/watch?v=miLqkSU_qG4&t=1570s
==============================
Channel Title: ekkol√°pto
Video Title: The Limits of Artificial Intelligence w/ Prof. Elan Barenholtz PhD & Dan Van Zant
Publish Date: February 14, 2025
==============================

TRANSCRIPT:

hey uh how you doing hi I'm Elon barels I am the co-director of the machine interception and cognitive robotics Lab at Florida Atlantic University howdy uh my name is Daniel VanZant I'm getting my PhD in computational neuroscience at Florida Atlantic University and this guy's my adviser I'm kind of working on a little bit of a a sort of grand thesis um which is that cognition itself um is basically Auto regressive generation so what I mean by that is that the large language models like things like Chachi BT do this thing where they they're a trained Network that's that's has learned to Just Produce the next token in in the case of language is just the next word um typically it's just the next word and then uh after produce the next word uh it uses that to feed that in as as its next input and it builds up um a larger and larger input and cons and but does this stepwise so it's just taking an input producing the next token tacking that on to the the previous input and then producing the next token again and then stus does it successfully and what we see from the large language models is this seems to actually solve language um we can get more into that um if we have uh as as we dig in um but to me this was uh a kind of epiphany that perhaps this is what cognition more broadly is uh that what are brain is is a train Network that has learned over the course of uh our lifetime and experience uh what is the uh appropriate next token whether it's uh it could be language but we it could be in other modalities as well when we're thinking visually we might produce the next uh the next visual image based on the images that we uh have thought about uh in in in the past in the recent past um and I'm I'm sort of going after this aggressively uh as as kind of like proved me wrong aam's Razer that this might account for all of cognition that what we're doing when we're thinking and when we're reasoning is we're really just doing next token Generation Um and that knowledge memories and all of these things are really just the trained model so when I say uh you know remember yesterday when we had drinks what you're doing there is your brain uh is is maybe over the course of the evening uh when you were sleeping has has integrated this information into your brain that you you that we we had this experience and what that means what it means to have integrated is that when I say those words remember when we went drinking um it's going to generate that sequence of the visual sequence uh which is going to happen autor regressively and that's what it means to remember something and then if as I'm thinking about that uh and and I'm remembering that I'll I'll the linguistic system will be able to gather information from that and say yeah oh that's right uh you got uh you know Martini um and and then the linguistic system and the visual system will sort of act in concert and basically uh that uh synthesis is what we call cognition so that's sort of the the the Bold broad claim here uh that all of cognition and knowledge representation memory and all that uh can ultimately be accounted for as a kind of train Network that does auto regressive um generation absolutely so uh Alan said he's almost putting this out there as a challenge and uh in some sense I'm challenging it or at least part of it uh and that's that I think a really important part of cognition and a alluded to this is knowledge so it's it's things we know about the world and I have this idea and some experiments I've done that seem to indicate this idea that knowledge has its own structure and the structure that knowledge has is nothing like the structure that large language models have um and this is really important uh because not only does it mean that large language models would not be able to have knowledge in the same sense that humans do or even animals do um but it would also explain things like hallucinations or the fact that large language models can seem like they have split personalities where if you get them into one kind of conversation they'll say one thing and if you get them into another kind of conversation they'll say a completely different thing which is a very inhuman thing to do um unless the the person's schizophrenic or something like that and so I we can get into that a little bit deeper but but that's sort of the the fundamental idea is that knowledge has its own structure and that you can translate some of that structure into the way large language models work um but that you lose something in doing so uh the other part is that I think is important at a high level is large language models work there's some sense where you have a computer and you have software that runs on the computer and so uh I think part of what I'm saying and where I agree with Alan is that language is the computer so language has its own processing power language can perform operations language can do really important things for cognition but that large language models are missing the software that runs on the computer and the software that runs on the computer is where knowledge is represented and where information is synthesized and where uh all sorts of interesting stuff happens you know um and these These are you know obviously good uh good points uh important points I want to sort of address the last thing first which is I think there's a very strong intuition most people have and would share um that that language is isn't the whole story of course uh when we're thinking and conceptualizing so two things to say about that one is that the the the reason I was Gob smacked and I think we all should be um by llms well besides the fact that they're just extraordinarly impressive and are doing things that um you know I I didn't see coming um you know the scaling laws have really have been very the scaling is is has sort of solved a lot of the problems that that people uh imagined um but they're they're really running only on language uh that that's something that's very important to reinforce is that uh leave leave alive leave aside multimodal models for a moment although I don't think that poses uh an actual uh it's not really a Counterpoint um but let's just talk about text based only right large language models just churn through uh linguistic data and what they learn is just the patterns uh and the structure of the language itself without any exposure to there's there's nothing else in its in the in the Corpus there's nothing else in its data set uh that allows it to conceptualize Beyond just the the relations between words and you to me the big surprise was that that's efficient uh to do uh what's what looks like fair Fairly broad-based linguistic reasoning in any case linguistic reasoning about uh you know well call it reasoning call it whatever you want um they they are able to produce chains of thought um that look very much like and and really pass the smell test and and the sort of the rationality test um of of soundly reasoning and and responding in language and that means language seems to contain within itself the appropriate structure for generating itself independent of any other conceptual framework now in the case of human beings of course language is INS sconed within a broader uh kind of environment and that environment includes the ability to think non- linguistically and that I I'm not denying that and I don't I'm not claiming that language operates completely modularly if it of course it doesn't I I can describe you know this the black shirt you're wearing language didn't have linguistic knowledge about that it had to get there somehow um but that information uh ultimately ends up for the linguistic system as some sort of linguistic token and then I believe then it just runs within based on uh sort of its internal processes but at the same time when we're um engaging in in thought and we're we're talking about you know abstract ideas or or anything anytime we're we're we're frankly when we're thinking the language model is also acting in conert with with other um other faculties and I think this now I would call it being you can be one can be mistaken and and I'm saying on the basis of my of my thesis one could be mistaken for thinking that therefore there's a completely different kind of conceptual framework uh that isn't captured by Auto regression because we only know Auto regression works right now we only really know it works for language and we don't know that a similar kind um of of uh sort of computational framework is going to work for other uh other kinds of thought but the thesis is that if it works for language it likely works for other other Frameworks as well and therefore when we're talking about having ideas outside of of just language or just of a regressive uh linguistic like the kind of things Alam do I'm saying who says uh I'm kind of making the argument like are you sure perhaps we can account for all of those things is it just an intuition a very natural intuition to say like oh come on it's that's not all we're doing we know about abstract Concepts and I'm like no we don't what we've done is we've learned over the course of uh you know of our our developmental history to produce this kind of autoregressive process that appears uh to be doing something much more abstract but in fact perhaps it's it's closer and more concrete to the kind of rule learning structure learning that we've done in the that we LMS can do in the linguistic domain um now I forgot there was there was an earlier point you wanted to that I wanted to return to but I'll need you to remind me what it was cuz uh the first point that you made anybody remember Addie what was it well before we do that you're using the word autor regressive a lot you want to explain what that means briefly okay well so so I kind of mentioned this before um but I I should unpackage it a little bit more it's it's autoaggression is the uh the process where your your your output serves as your next input um and and as part of your next input so so let's let's you know think about in the context of of chat gbt which a lot of people are very familiar with you you type in a question um you know here's here's the question um you know what do you think of of uh the idea that U that the brain is just an llm right and there there's there's the input is just that sequence of text so uh what what the uh llm is going to do it's chat gbt or Gemini it doesn't matter um they all operate uh based on on the same uh same basic principle it's it's going to say given that set that string uh you know what do you think of the it's going to produce just a single token I like let's say the next thing is I think or as an llm whatever it's going to say it's just going to produce that very next token let's say it's as right as an llm I I you know it often says things like that um and so that as is now going to be uh tacked on to the previous sequence which is the enre question um there could be some invisible to the user kind of uh uh other tokens that are in there uh that shows sort of this is where the the uh you know the the user stopped and this is where the LM started but it's taking in that next token that it generated it's feeding as the input and it's going to say as okay given the given the previous sequence and the word as what's the next word and it's going to say as n and then it'll continue to do that uh successively and that's thinking uh for the llm that's the only kind of thinking Can it can do um but turns out that that is sufficient uh to be able to not only do next take pred prediction based on the previous five 10 tokens but it uh with a large enough context as it's called meaning the the entire the size of the of the input uh the basic that's getting churned through uh on each successive generation uh it can think about think about uh things you know Concepts or whatever uh tokens anyway let's just be concrete that occurred very far in the past you could give a document uh that's many paragraphs long you could give it enti some of them if the context is large enough you can get an entire volume a book size volume and what it's doing is it's guessing this very next token based on taking into account uh something that was uh you know many chapters ago um and that's what we as humans do all the time in this conversation I'm making reference uh now to you know the the uh comments that Daniel made I I just failed at that by the way I was a not able to retrieve there's some sort of Decay that happened there or something like that but uh I'm taking into account things that have been said throughout this whole conversation but I'm doing it at this discreet this and again this aggressive it's just this discreet next token prediction and so the power of that uh that that ability to do next token and then just do that successively uh is is what I find astonishing um but seems to be able to to to reason and conceptualize uh far beyond what I would have assumed just based on that simple kind of process and so that's that's really autoaggressive now UT regression again it could could apply to something else let's say I'm thinking about um you know uh what I did this morning or something like thinking about you know I first I had to go on my computer and then uh you know and I I then I I got on the phone and I was coordinating with these guys and as I'm doing that uh I'm I have the experience of kind of going through a temporal sequence and the idea is that you could probably model that same just like the the in in the language context what I'm doing is I'm taking the previous imput so here's you know the the the in my mind I'm thinking of all the events that took place up to this point and then I'm remembering by what was remembering means in this case is just generating that next token and then I got my phone uh and I'm doing the same thing so that would be sort of large what Auto regression would mean is that what you're doing is constantly just generating uh sort of the next slice of of time um or or linguistic token um you know this would kind of VIs ual tokens in in the visual case um and then you're you're just tacking that on again to the to the input and the input is just growing larger now in the case of people I don't think what we do is is similar to llms I it's clear that there's some sort of Decay function or something like that large language models take literally the entire sequence and RAM it through again and again very very inefficient in some ways and and by and we we're without getting too much into details every time you do that you literally have to you have to process every every single word relative to every single word which is a very computationally expensive thing to do um the the gpus the the the compute power is there um although it's not probably you know it's not efficient to the point where ask open AI they're not making money uh nobody is I don't think correct and it's too computationally expensive um but in principle it's sort of I think it captures the Ence what we do um and and in humans it's clear that we don't retain the entire you know I don't I don't remember uh what I said you know really the verbatim uh you know even 30 seconds ago but something some version of that uh where we we we're somehow uh capturing you know the it's probably transformed in some way but it's still probably this this Basic Auto regression so that's that's the core idea of what autoaggression is and what I'm what I'm proposing accounts for for all of it for for cognition and so knowledge in this case um isn't organized in any um way that's that's not captured by the the the representation the pattern of um of of how pref tokens have appeared together uh I guess you could say um so I I and I I don't I don't know what your conception of of knowledge is and I'll hand over the mic to you um but yeah the aams razor challenge here is given that knowledge in the case of LMS is just the ability to produce the next token in such a way that they sure sound as if they know a lot of stuff right and there's nothing else in there but just the representation of uh you know the relations between words and it gets very semantic as you get deeper into the network it's not you we can't just talk about like literally relations between words they there're it's much richer than that the representation of these words is is a rich very rich thing um but fundamentally that's sort of the claim it's not more organized than what the LMS are doing so not or not not organized in a very different way yeah yeah I see what you're saying um so I think that's actually a great segue into the first point that you asked about and what I wanted to talk about so I mentioned uh I think knowledge is software and it oh software that's software that runs on a computer that we would call language um and and so it's language isn't the whole story uh and in particular what I and something I want to mention really quickly that I was thinking about when you talked about is uh I think a strength to your point and maybe something you should start mentioning is uh language isn't the only autor regressive process that generative Ai and humans share I would say that producing images is also an auto regressive process for both no I I did say that oh you did say that I did I make that I made that claim I just said uh so I think it's worth mentioning that humans generate images autor regressively where they throw out like a rough sketch and then they fill details interest okay not just speculation I see what you're saying that when people do uh actual image generation they sort of do it auto when people draw a picture they do it auto aggressively and and when the the image generation models that exist draw a picture they do it auto aggressively where they kind of it's different but they kind of make a rough sketch and they fill in the details diffusion okay so I was going to uh so when I think of imagery I'm thinking of like like I think all of our imagery is is is uh is video video generation right we don't really do image generation as people as humans in our mind um now we we have to do it when we when we're drawing um but I you know when I say alluded to earlier and but it's more of a conjecture is that our I think our our visual thinking like a memory um you know episodic memory I think could be accounted for as a kind of uh autoaggressive video generation similarly visual reasoning like thinking about how do I get from this point to that point I I think if we're doing if if it takes time for us to do it like cognitively like I my suspicion is that what we're doing is just this that's why you can't do it instantaneously certain things you can just solve like in moment of insight uh recognition boom I've done it um where are some things we have to think when we're thinking what are we doing and my suspicion is that what we're doing is we are actually doing uh this this sequential UT regression now I think that the video generation models um I'm not well vered enough uh to know if this to what extent industry uh is you is is uh homogeneous on this point but I know that many of them do use Auto regression um and that would be essentially the same idea produce next produce a frame take into account the previous frames like start from this point uh then produce the next frame then take the previous two frames produce the third frame and so on and that's classic autoaggression and and I think that that is very likely the same exact sequence that's happening when we do visual reasoning now your your point about when people do it like sort of actually generate in the world our actions are also in this way kind of utter aggressive when we're reaching for an object uh and it might be I don't want it to you know it can border on the topological and you have to be careful I'm making a very specific claim that that the the computational process really takes into account previous States um and that those are used to generate the next state you know when you're reaching for something it might not be autoaggressive in this way but it's uh but it very one might be um I I I haven't been able to to De to delve deeply enough into it um but certainly when we're acting and when we're generating like generating we're we're doing it sequentially um it's certainly true but the world operates this way like sequential ism is sort of built in like like it or not one moment follows the next it's just how it's physics it's like the world whatever you know I'm not going to get into the we're not going to go too deep here but you know the the world sort of unfolds and and and over time um for lack of a better word um and and of course sequentiality is there but there's a there's a there's a sharper Point here about to the computational process as to whether it's autoaggressive really it's building up a longer stack um of increasingly um uh larger input that then forms to to do the next uh to to to form the next output um so so I you know um I don't know if I could point to to to your example as necessarily supporting it because it might be that that's really just happening I you know to use the marov and uh it's like is it taking the past into consideration or not um autoaggression absolutely has to take the past into consideration Marco movian processes do not um it could just be the current state is sufficient to generate the next state and I don't know if drawing necessarily requires that you've taken the previous States the world takes care of that the world uh is in capture you know by by the virtue of the fact that you've made marks on the paper now you've got instead of just a little point you've got a curve right but that could just be the next that that's not part of your cognitive process that's taken into consideration it's the world is changed it's similar to uh to uh cellular aoma um which which um in some ways sort of seems autoaggressive but it's not um because this the the rules just say give the state now they're totally marvian uh the the the the state of the environment uh in the cellular automtic case is keeping track of the past such that the the world sort of takes care of of of the maintenance of uh and so it's sort of uh in in in totality you you get sort of Auto regression strangely enough but it's not computationally in terms of the the rules yeah uh for Generation it's not aggressive the auto would you say the in cellular autom the auto regression is an emergent property that comes from the local properties of the cellular autom plus plus the environment the en plus the environment which has memory right the the environment has its memory of of sort of what uh you know the changes took place are are preserved yeah and that defines the state for the cellular autominer so any sort of f sort of you know State it's just uh you know I guess it's a finite not even a finite St machine I I don't know how forget jargon um right especially if you're not you don't know if you're using it correctly um but but it's it's just uh you know the rules are only take into consideration it doesn't you don't ask what happened in the past m in in a in a in a celular tomon you don't uh the rules do not ask what happened in the past in a autoaggressive uh generative model it very much inherently does it's by just by virtue of uh the input um is I mean in some ways it's it's not that different and i' I've been thinking about this but shouldn't do this on camera but you know there there one is sort of an internal process where you're building up this the the the state is just this increasingly accumulative situation and one is it's the world uh that's doing that um so they're they're very they have certain very strong similarities so you're you're saying that an auto regressive process has to account for multiple States it can't just account for the previous state it that would be a Marian process it needs an increasingly uh well it's just that the the state itself um is dependent on its own outputs in such a way that and and and and it's sequential this is very essential um so this is the big difference right so in a in in a cellular tomon there's no record in the world as to how it got there so how what right now you've just got and I don't know if we have we haven't explained what s tomon is but let's just let's let's let the listeners you know just look it up because just look up Game of Life really cool yeah so a cellon is basically a it's it's a process for taking um some simple rules that say let's say you have a grid and a grid uh can be made up of little squares and the squares could either be black or white um and then what you do is you have a rule given uh so for for a given Square uh you say okay what are my what is what's going on in the surrounding squares that's going to determine an update there's an update rule that says given okay let's say I'm surrounded by black squares I'm going to turn black or I'm surrounded by squares I'm going to turn white um and uh basically what happens in in celom is is you have those rules and then you do this for a complex grid with uh you know whatever pattern and then interesting things happen um and and it evolves and so you can get for example you can get certain kinds of patterns that recur um and certain s of stabilities uh that can emerge um as you run this thing um and the point I'm making is so there's a certain sense in which it's like the the the past leads to the present um and and and it's certainly the case that the the environment is doing this this kind of evolution but first of all in the computation itself uh the rules do not specify what happened you know three moments ago it's just whatever happened whatever is the situation right now here's what we're going to do next um in the case of large language models the sequentiality the sequentiality the the is is built in the the input uh that we keep feeding so let me just back up so right so the input in the case of cellular is the the current state status of the grid the input in the case of llms is the previous sequence including whatever I've generated that's tacked on um most recently now so these could seem very similar like we've got the past led to the the current existence of this grid and the past led to the the current uh sequence but the difference is and this is fun absolutely fundamental is that in the case of language models the order of in which those words where it's produced is preserved in the current environment the current input whereas in the case of the uh celom the order is not preserved uh it's just the current state how do we get here and that's what makes it really Mar Co it's does does does there's nothing there's no information about the past that's that's built in to um to the current state whereas inherently in the case of of language models uh the the the input contains all of the history about the past and the on the previous States so uh that's that's really a certain sense of which uh the there's um that auto these autoaggressive things they're they're different then then the envir when the environment keeps track that's not the same thing the environment doesn't keep track of the entire sequence that happened it's just really just the present and the claim is that what we're doing our brains are doing is we we actually are we have to retain a history a sequential history in order to do the next thing I see I see and so I think this is a so this is good in specifying where I'm disagreeing with yeah yeah that's what hoping to get to that I'm sorry yeah yeah yeah no no no this is all been good discussion but the where I disagree with you is that uh Auto regression is all that there is and there's just sort of the entire history with maybe some Decay function to drop things out fed in and then we generate things and and that's how that's inal that's that's there you go um and so I think and my view is actually the the conservative one um which is funny with you being the adviser and older scientist and me being the younger scientist but but my view is the more conservative one and it's that there's two systems here um and these two systems are very similar to what a lot of cognitive scientists have thought for a long time which is that you have that's I know you're wrong no I'm kidding which is that you have long-term memory and short-term memory or working memory and these are two separate processes and in some sense uh short-term memory operates on long-term memory I think where's absolutely right is that auto regression is the core mechanism for working memory or or uh sort of cognitive functioning or whatever you want to call it the things that go on in your your prefrontal cortex and yeah I I think they go on in working memory seems to be kind of broadly distributed like well when you yeah it's prefrontal maybe well let's it's it's it's it's controversial uh as to you know how if there's like a coordination but let's leave that aside it's it's a nice abstraction to say this process happens here even though it's not true so we should be clear uh good neuroanatomy is uh there's this one of my favorite papers is called uh functional neuro Anatomy is modern phenology and um let's let's be clear that that's very don't get me started but I think when you're trying to simplify things it's still really useful to say this thing happens here and this thing happens here and then this thing moves this information forward my blood pressure a little bit but okay we'll talk about things with that way knowing that that's not how the actual brain works and there's lots and lots of issues with that but it's a useful abstraction I think and maybe that's why it's stuck around for so long yes it's good point right if if it's sticky then maybe it's useful okay yeah yeah if it's if it's not what is a I had a mathematician who always used to say uh he used to say well this is how the universe works and then he would say well and if it isn't true at least it's useful so if it isn't true at least it's useful or it's leading us down the wrong PA but anyway at any rate drink theorist in the comment I do have a comment though on experientially right I mean it's interesting that an idea pops out to you a thought pops out of you out of somewhere yes not actively thinking about the thing and it pops up and and I wonder if you wanted to and and that's that would seem to be like sort of a challenge to to my perspective right it seems completely unpredicted uh it doesn't seem like the previous tokens would would generate that but I you know my my argument would be like says who um you know if you spend some time with llms you can find that they they can surprise you uh I wouldn't have gone off on that tangent necessarily it goes off on a sort of a tangent uh that I wouldn't have said that would be the next token I would have generated um and I I I think the richness you know what we've seen what we know now is is that the the the the richness of the auto regress Auto regressive um process is is is extra inary um and maybe you know if you're taking in totality all of the and we're also adding by the way not just linguistic when you have that that thought that comes from nowhere um seems to come from nowhere there are other processes that might be going on at the same time as just your you're sort of like thinking out loud or thinking in loud whatever you call it like thinking to your to your within your brain uh linguistically but you're also thinking visually like it it you know the predictiveness it's very hard for us from a metac standpoint to be able to say that that Insight came out of nowhere uh there's some other process that's the argument would be like says who uh maybe it is predicted in some way in ways that we're just not able at a medic cognition cognitive level able to to to be able to to grasp but that doesn't mean it's not the same process I don't think that that that's a fair conclusion um what do you think so so what I think is is that there's there's two things and and there's uh there's working memory which I think you're you're absolutely right I think it's Auto regression but I think there's this other thing yeah I think there this there's this other thing that's long-term memory and uh there's a few reasons that it's important if long-term memory isn't anything like large language models but let me give an example of what I'm talking about and how I'm thinking about things so uh there were two people who have been going to my church and in my friend group for years that were siblings and I didn't realize they were siblings uh and so I had talked to one of them and he briefly mentioned that his sister uh also goes to this church whatever I didn't think much of it I don't know everyone I was like okay sister also goes to this church um they these two people look similar to each other but people look similar to each other all the time that doesn't mean much and then I was talking to this girl and she said uh oh yeah my brother goes to this church and in that moment I had three facts in my long-term memory I had the fact that his sister goes to this church I had the fact that they look similar to each other and all of a sudden I have the fact that her brother goes to this church and so how I want to imagine things is those are three facts and I imagine they're stored in in something like language maybe language with with a graph over the top of it and connections between them and when I had those three facts together I asked her oh is your brother this other person and the reason I asked that is cuz like you said it clicked and and how I'm imagining things is through some mechanism these three things got loaded into the autor regressive process M through some mechanism that I think is somewhat separate from the auto regressive process but these three things got loaded into my autor regressive process my auto regressive process was able to make a relatively small jump and reasoning about it at that point and then what's really interesting thing is the auto regressive process changed my long-term memory because I load loed in three things it generated a new thing and then all four things got loaded back into my long-term memory she looks similar to him she has a brother that goes to this church he has a sister that goes to this church they are siblings um and so you sort of updated your your your memory yeah in a in obviously sort of a structural way um that you feel like wouldn't be well captured by whatever it is that that these neural networks are learning um by virtue of uh you know churning through this data and learning to do ultimately next token absolutely and and the reason I feel it's I think it's interesting to talk in specifics about the reason I feel like it wouldn't be wellc so I feel like the whole system is something like uh what we call a graph and a graph is essentially you have you have nodes and you have what are called edges and uh you have this thing and then there's an arrow pointing from this thing to this thing that means that this thing affects this thing and one this one direction yeah MH um and and so what I talked about as a great example of this where I have three pieces of knowledge and kind of I how I think of how that stored in in long-term memory is almost as a graph where all of a sudden I have arrows from these three separate pieces of knowledge to this new piece of knowledge that I created with my autor regressive linguistic process and then I go and store that back in working memory and we can imagine these pieces of knowledge are connected to a host of other things and uh so I I mentioned you know they both go to my church so that's connected to a whole other body of knowledge about who are the people that go to this church and and who's the pastor of this church and where is the church and and space and all sorts of other things right um and and there's all these arrows pointing from all of these sort of small beliefs to other beliefs and this is really important if this is a graph because uh one way to think of large language models is that they're a hyperdimensional space I'll get into what I mean by that essentially there's something you can do with a lot of things that's called uh embedding where you take something that's structured in one mathematical way and you put it into a different mathematical structure so uh everybody seing a map of the world uh everybody seen a globe a map of the world does not look like a globe it's flat it distorts certain things because you have to if you're going to flatten out a sphere uh but one way to imagine that is that you are embedding a sphere into a flat plane and so you're you're sort of translating each location on that sphere to a location on the flat plane and you can sort of you can still perform a lot of operations that you could perform on a sphere so you could say these things are this far apart and these things are this far apart and you don't need the sphere anymore you've embedded it so there's two types of ways to embed things uh there's what's called called lossless so that means I can do everything perfectly just as well as I was able to and then there's what's called uh lossi embedding so and I mentioned that the distances change if you turn a sphere into a flat map at least the normal Maps they're special maps that are weird shaped and you can perfectly preserve distance with but most the map you've probably seen the distances are changed so unless you unless you know what the original transformation was the the information is actually lost the distance the correct the original information is lost and if you want to get a ruler you can sort of measure here's this location this location here's this location this location and usually the way flat Maps work is it'll work on a small scale but if you try to do like here's California to Iceland and here's California to Germany it'll be way off um here's how big Greenland is yeah yeah and and so uh so exactly so so this is lossi and you can also embed the kind of structure I'm talking about which are graphs into hyperdimensional space which are the type of structure that neural networks are based on so neural networks are a series of transformations in hyperdimensional space I won't get too into sort of what hyperdimensional space is because that's another conversation but just imagine graphs are a sphere hyperdimensional space is a flat map and to translate one into the other you have to lose information and actually if the graph is very complex you have to lose quite a bit of information and so the case I'm making is that the knowledge capabilities so like if you go into llm and say the cat is the opposite of a the llm will say a dog and that appears to be knowledge I would say that the knowledge capabilities that we see are the large language model having a lossy version of the kind of Knowledge Graph I'm talking about in its hyperdimensional space but because it's lossy we also see these kinds of issues at the edges the same way I mentioned where if you're using a map normally and saying how do I get from uh Florida to North Carolina it's going to be fine to just look at the distance with the ruler but if you're trying to do other things with it it'll fail if you try to do some of these other things with large language models they'll hallucinate or you can get them to State completely different things depending on the kind of conversation you have and my argument is that these failures are because of the lossy embedding of a graph into this hyperdimensional space and it's because the large language models are trying to do something that's not natural to the structure that they have um so they're doing it and and they're if you get if they're large enough I mean there's as many I think we've reached as many neurons as a human brain are close to it so if it's large enough you can really get really far along with sort of saying well this isn't quite how I work but I'm going to truck through it anyway large enough and also um have enough data I mean they're they're they're exposed to ridiculous relative to humans so there's there's of course you know very very significant differences right now so this actually a point and I remember the point I wanted to come back to earlier about things like hallucinations um and and and in general um whether the current state-ofthe-art and the current ex uh performance of these models points to a difference in kind or uh is is it or is it just are we are we some fixes away and and we're we're on the right uh sort of theoretical conceptual path or you're making the case that no there's something radically different going on in in the case of of people and I think this is sort of the the the core um discussion here uh you you kind of you you sort of detect in the kinds of mistakes they make something that's that's extremely not human not humanlike and I think anybody who spends a decent amount of time with these Elms will have those experiences where you're like that's not just like you're missing a fact which people do all the time or maybe um even contradicting some belief that you say you had but now you're saying something maybe that's not consistent with that belief but it seems like there's like like who was I just talking to you just said this um I said you know don't let's tell don't repeat this and then it repeats the same thing like don't please don't repeat that and it just repeats the same thing like what's what on Earth is going on this is an extremely unhuman type of thing to do um I mean it's it's not the best example of of the kind of things you're talking about but I think this is sort of where we're at um in sort of the the empirical landscape is we certainly do see these kinds of this dissonance uh these these these uh cases things like hallucination now people hallucinate too um but it's it a different it is a different kind of hallucination I think most of the time they realize they're hallucinating which is really key so I mean I know people who make stuff up and they kind of seem to believe it uh in a some sense uh that that almost looks like it doesn't really matter what their actual past was like this is the the network is generating this now um and I don't know to what extent I guess that's that's where I'm you know I I think I think there's a lot of room here for making more precise specific claims and and maybe maybe you done that or maybe you can do that um about what kinds of excuse me of hallucinations uh exist in in the networks uh in in the in the models that that we would never see in humans and they're just of of a different point part type or or what kind of contradictions um you know things like now you know these three people are their siblings or whatever uh you know that realization and that sort of percolates to the rest of you know how you're going to talk about these people and how you're going to think about them maybe you could argue that that you large language models don't do that U they can learn you know you train them on some new fact does that successfully percolate in the way that you would expect that in your in your system there's there's sort of hierarchal knowledge graphs um and then that percolation sort of maybe comes for free is like there is a central node that's connected to a lot of other nodes you update that node and it and it and it changes the the behavior more broadly maybe the claim would be that we don't see that kind of in in the in these models we haven't really talked much about what it is that they're learning you if Auto regression aside what happens in these models though is that they are churning through enormous amounts of data and what they're doing is not memorization they're not and that's they're not regurgitating people have this misimpression that what they're doing is predicting the next token based on that the fact that they've seen this sequence before that's not how it works at all language can't work that way because every single sentence the combinatorics are too big are you know previous 10 words were probably never uttered in in the history of humanity unless you're reciting this the Pledge of Allegiance you're not going to have the exact so they're not simply regurgitating what they've seen they are doing something much more similar to this this so there's this embedding that's happening um in across many layers and they really are abstracting away for lack of a better word um what these tokens sort of how you represent them the word doesn't it's not just the word and I kind of alluded to this a little a little bit earlier right and the the word any word that you use in within the model as it goes into the model it doesn't just remain that token uh in in the sort of very very Bas sense and it's just trying to guess based on this a very important point it's not just trying to guess based on the pattern of these tokens per se it's turning these tokens into this much richer and embedding and and and with the Transformer models those tokens uh the word the way it gets represented is combined with all the other tokens um and and that that's really that's what attention is uh the ultimate representation of any token any any piece of the sequence is combined with all the other members of the all the other other tokens um and sort of structured that way and so it's not this dumb system uh that we can just easily conceptualize and say it's it's just taking this pattern and it's producing the pattern these are hyperdimensional representations of the original pattern um in such a way that and when it's learning it's doing that same thing it's not learning the sequence of tokens just like that just the way they appear it's learning them in this much Rich much much higher dimensional much richer and com and combinate uh I know what the word is combinatoric structured again the idea that the that it's structuring the the individual tokens as combinations of the other tokens this is all built in to the way it's representing that information which is why it's really doing conceptualization it's not doing what we think it's you know at the simple level it sounds too simple um does that end up looking like a graph um in some sense uh there it is sort of a it's it's a directed graph um as you go from layer to layer um but I don't think that it captures it I I think I think we are I think we have a disagreement maybe we're closer than we would think uh you know in terms of what it is it's doing when it does that those hyperdimensional embeddings that it ends up and and and it is against sequential across layers it may end up being captured uh you know sort of with with a kind of graph theoretical uh framework but I think we're disagreeing on whether it's sufficient the way the llms are doing it yes will ever be sufficient this is a good word I I think in technical philosophical terms you are saying we both agree that auto regression and this kind of process that large language models do is necessary for cognition certainly autoaggression but what about the what about the you know the train of the model M uh where you're set you're determining the weights in the in the in the in the Transformer model um that's this embedding right there's embedding plus plus self attention um but the embedding happens with self attention it's churning through the whole thing so it's doing the embedding such that it's going to be good for the combinatorial self attention mechanism this is very very very abstract complex stuff that's going on it's not just you know simple uh again pattern match could that be sufficient mhm to to capture the kind of knowledge representation that you're rightfully pointing out certainly we do um now does the fact that there are deficits now in the current system say that we're no it's doing something radically different um you would say maybe make that case you have other ideas of how you can do it computationally that you you say are are inherently absent first of are we sure that whatever it is you're envisioning isn't actually sort of phenomenally happening in the the entire embedding you know layered embeddings and all of that well I I think I'm actually sure that it is happening I just think it's happening in a lossy way okay um so well so maybe our so maybe our our point point of contention is uh may not be that large so so what do you mean so L is opposed to I I think there and there's sort of there's all sorts of interesting things about how you you can represent neuron is a hot field Network and other things you can transform neural networks into but I think uh I think what I'm saying is that I think you have to have some kind of Knowledge Graph structure of for memory and knowledge and things like that I'm saying I think the large language models have some kind of Knowledge Graph structure they do um sort of embedded in the neural network but what I'm saying is and and by the way the reason we keep talking about deficits we're talking about really clear ones like hallucinations but some of these deficits are like fundamental reasoning capability so this isn't just an academic Pie in the Sky conversation this is like you know there's reasons our our robots aren't smart and can make mistakes and shouldn't be performing surgery or PL yeah so this is an important conversation very important um it's it's like maybe the most important as far as these models go is like is are we getting to like utility where they can do the stuff that we want them to do reliably I agree that's it's a very very fundamental yeah yeah absolutely so so I think there are so I guess what I'm saying is there's there's two systems and and and they're sort of the more graph based system and then there's the autor regressive system and we've built an auto regressive system that's large enough and that's been fed in the entire internet that is sort of bodged together the second system that it needs inside of its hyper hyperdimensional embeddings um and and so it's I think somewhere in there you would you would find something like a Knowledge Graph that's embedded in so so I want to stop you real quick so you know when we say it's an autoaggressive system Auto so there and and it goes back to the software kind of a point maybe not exactly software but shortterm memory along long ter the the training regimen that it is trained to do auto to do auto regressive next token generation but in the process of doing that training yes it is forming this very very very very rich hyperdimensional all of that uh you know sequential uh you know we could even say gra you know it's it's it's it's a graph um representation of all the information in order to do the next so the autoaggression is sort of the task and it's the it's the it's the it's the environment in which it produces it's able to form Its Behavior um I think what it learns it learns to do the autoaggression which is which is in other words it is fundamental but at the same time that doesn't that doesn't that doesn't limit yeah every everything you just said about these are doing something that's not just next token prediction it's we agree on by the way well no but but I saying it is just learning to do next token generation but to do that it's got to learn about the whole it's got to learn about the world see what you're saying and so it needs and and so and we we see that that's why it's so it's so surprising on some level um I when I first I kind of remember when I first like that's I learned that that's what they're doing I was like that can't be right like I thought somebody made a mistake uh next token what do you mean like how could that encapsulate uh contain within it all the information you know how could that be the the key um but the answer is that in order to generate next token you've got to organized you got to organize your knowledge of the world sufficiently that you end up learning Concepts you end up I mean it's still just next token generation it's what it's learned to do but that that the the train Network which is why so the brain you know sort of the I think it it's like uh you know long-term memory is just the the the sort of the it is is is the trained Network this is how I think of it right the trained network is it's a set of Weights mhm right and the set of weights at this moment is just some system that if you give it a certain input it'll produce an output single output that's the brain it's this trained Network the process of cognition is that trained Network in an autoaggressive environment so that when it produces the next token that's fed back and then it and and it proceeds from there I see but I'm not really making a strong well I am making a strong statement about how the brain gets to the state it does but the state itself the the weights can can contain within them all of this richness that you're talking about so it's not a not a limiting factor to say that is autoaggressive that's the environment which which it produces and that's what it's trained to do but what you get in the end is a very rich uh you know uh representation of all of the information that has been fed in order to to to learn how to do that next token so I think we might actually be agreeing here are we going to hug at the end I think we might be agreeing here uh which is that in the process of doing the next token prediction it learns to simulate the whole system and what I'm talking about and what a lot of cognitive scientists besides me by the way would say is a sort of a almost a two systems that interact with each other it learns to to have both of those systems sort of the the generative part of the system and then also here's a stored Knowledge Graph and here stored knowledge I'm saying there's one system right I think I'm claiming that that it's one thing that just operates in just like the operates in this Auto regressive environment um but you're saying you're saying there are two systems are we disag let's let's get to the point so so so what do you mean by two systems Transformers are Universal function approximators sure I'm saying there's two networks in general yeah yeah so so Universal function approximator just means if you have a big enough one and enough training data and enough training time you can approximate any function so it's a touring machine or whatever yeah to to an arbitrary degree of of error um nobody thinks the brain is a tape with the LI head moving back and forth you know the fact that that I I I find that a curious an interesting point but I'm not sure that yeah but but go on sorry so so what I'm saying is at its heart cognition as we think of it is a two- system process where you have one system that's sort of Auto regressive generative in the same way that Transformers and and Transformers are able to do this very naturally cuz fundamentally that's what they are there's a second system which is a more static system it's something like a stored Knowledge Graph and then a lot of what we think of as cognition it's taking things out of the second system putting them into the for first system doing some transformation and putting them back into the second system and so you've almost got like a warehouse and then youve got a guy that gets boxes out of the warehouse and he works on something and puts the box back in the warehouse got it got it okay and so what I'm saying is is I think because Transformers are Universal function approximators they've managed to capture both systems to some degree because they can capture graphs and you can loss lossy but you uh you can embed a graph into a hyperdimensional space which is what Transformers are but what I'm saying is that they're not capturing the second system this graph based system very well and that the fact that's why they make the mistakes and that's why make the mistakes they do and that um the second system is is really really really important for a lot of advanced reasoning capability especially reasoning over very very large amounts of information so I think a perfect question that it's really hard to get large language models to do even if you hook them up with rag which in some sense if you do that want talk real quick what's rag rag is where you by the way I think this may might be a point towards me rag is where you give the large language model a second system that stores information MH and and there's a lot of technical detail there but essentially you give it a second system that stores information and you can take information it does I'm working on that I I certainly can't I'm not going to hold you to account because Rag and it's current Incarnation like you know it's like six months old a year old I don't know uh like I'm not I'm kidding uh that's not really a point for me but it it works better for a lot of things than a large language model by itself I think um and and I think that's the important statement so so rag is where well the re okay but let's let's I think that's a great Point um to to jump off of a little bit so why do you need Rag and and the reason is um there's there's two ways to get information sort of into well now there I guess there's three uh right there's there's three ways to get information into a large language model you can train it which is mean you're updating the weights that's the sort of the brain that I I was talking about that's sort of the static system um by virtue of of uh here's here's here's some text here's what you should generate based on that text uh you know and it just like any neural network um it will try to guess based on you know here's here's a pledge of allegiance it's never seen it before um what should the next word be uh you know I I I pledge allegiance to the it's never seen before it doesn't know I pledge allegiance to the god I pledge Alle the right but then you say no it's Flag uh and now it's learned it and now next time it comes through uh it will it will get the right output um so that's training and you've CH change the weights the second way is uh you can do it in context meaning uh when it's having the conversation it's going to learn it's quote unquote learn but it's going to go into the Thee the the this context it's going to go into its inputs as he going along that's how you can tell a large language model um hi my name is Elon and it'll say how's it going Elon um it's not that it's put it that name it's not taken uh you in it's not taken that fact and put it in some other stored knowledge base um it's just in the context uh and it's in other words it's just part of the the the the input that it's going to keep um Auto aggressively going through that's number two and then number three is kind of the stuff that uh Daniel's talking about where you can have a a knowledge database that the L language model can try to access now it's the autoaggressive system isn't really doing it you have to have some other process and retrieval augmented generation is one of them um augmented here is so the generation is just autor regressive generation the augmented is that um is that you're you're taking the the the current uh you know whatever the the uh input is and you're saying based on that uh I'm going to go look up and see what might be relevant in some other database and it's a it's a distinct separate process and I think this is the key uh so why I elaborate on this so much it's sort of the key of the of the the debate um is there knowledge stored externally to the sort of trained model and then the train model goes and retrieves it or I don't even say the train model it's not the train modelet it there's some process that goes based on whatever is going on in the current uh you know conversational context or thought context says let's go retrieve information from that other thing pull it in and use it right that's what retrieval elemented generation does you're saying yes there is this other database that has to be accessed I'm making the you know sort of it's may be a devil's advocate argument but I don't know uh that no that doesn't need you don't it's all the strain model and that what knowledge representation is it's when we when we you know consolidate and and you know when we're dreaming and sleeping and whatever uh all of this is is just a way of training the model it's much it's it's really just a training process rather than an an external separate now do I think that training process leads to exactly what llms do that's not the claim right in other words could it I don't know what the structure no and and it's not a claim that the particular Transformer structure is the right is the right but in the end it is a substantive Claim about what you have as a trained model rather than some that is a TR metal plus an external representation and that I think is the the point of I that's the Crux that's the Crux is it a two- system or one system process cuz I think we both agree that that system the autor regressive system is important I'm just and very rich and complex and and has sort of conceptual knowledge right all of that stuff okay yes yes I I think we completely agree on that point my point is that you still need this other thing so the autor regressive system is necessary but not sufficient after an hour of talking I think we figured out what we were arguing about that's fantastic we should now so let's scrap that whole thing start from there that's record time for us by the way but okay now now we now we have have to fight about it but um but honestly I don't know that I have that much more even to say substantively um about who's right here except maybe a call an Article of Faith to me it seems like the brain is just one thing and it's just a big fat neural network and I think that given that we we have a glimmer at least of a system that seems to do the whole knowledge thing maybe poorly right now um but well enough that I would say I'm going to go out on a limb on a sort of theoretical limb and say let's pursue the theory that it's just single system and then I put the ball in your Cod and say no you know you have to give me the the argument is to know everything I see you know tells me that or there there's substantive reasons to think that it's the wrong direction by the way Alana is saying that the brain is just one thing because neuroanatomy as it stand sucks and and the divisions we make in the brain don't work very well but it's worth mentioning that in my opinion this is also something we disagree on I think the brain is is a few different things that have sort of and I don't really disagree course there are there are you know nuclei there these clusters that do s there's peduncles that are sort of Highways between the separate portions and corre like these and so it's worth mentioning that Al's making a controversial claim there even there that the brain is just one big ball of fat well I mean it is except there are you know it's just that it's topology is not simple uh in meaning you you know you have dense closely highly connected regions and then like you said sort of path Pathways to other regions and so the topology you know it is a network right it's it all is connected one way or another correct and there's there's certainly you know directionality like things go one way not the other and so you know it's a graph and all of that um so again I don't know you know that that claim isn't really a controversial one it's really more conceptually do does does labor get divided up in such a way that this model of just next token generation even though it's it's recruiting all these different systems isn't really at account like computationally it's not the right account so biology sort of aide like yes it's it's almost hand waving to say it's one big Network or it's not um you know even large language M also there's there's all kinds of I'm sort of fascinating topology that that happens sort of semantically even though it's all just you know the connection but the connections get zeroed out I mean a lot of it's just not you know meaningfully in some ways things are not always uniformly connected and uh there probably is interesting topology there too um of course it's not as simple as just saying it's it's all just one big brain um I'm not making you know the sort of the the uh very simplistic version of that argument but computationally are these things being recruited in this in a way that's captured by what uh you know a train network doing next generation or not and you're saying no uh there's a different computational process it has to be described differently uh it's not a multifaceted trrain Network and you have to have this other process right right I think that's sort of disagreement um it's worth mentioning your idea is more parsimonious it has less moving parts means that until you're proven wrong science is really on your side my intuition is that this is wrong but as as a scientist I I have to go out and prove it and I'm working on that well I mean but on the flip side you know you can point to deficits in these models as you and and it's a it's just it's let's face it it's sort of subjective it's kind of a gut shot gut call how to do science on these things yet not even close and and when in in absence of you know really any science of it what we can say is like I I you know I could I can make the claim that it's just a matter of time and and and Improvement um and you could say no uh it's never going to get there on this current trajectory and we can't really resolve this except empirically and that's why yeah science is cool CU like but I think I think it it opens the the door to like this is an interesting debate I I and I'm not just self-servingly saying yes this is an interesting debate like I think this is something that you know research uh hopefully can you know certainly on the on the comp on the on the engineering side people want to solve problems like hallucinations and maybe you've you have insights that can solve those problems and you're saying this is an engineering solution I'm also saying and we're both saying this is also I think a question that has potentially M very large importance to understanding humans and our brains um and I think those are both true uh in the sense that this both presents an operational practical question and also a theoretical you know basic science question um I think we both need to uh we need to go back get into the lab yes we do and and we're in the same lab so we're going to pursue uh you know opposite uh opposite and that's perfect I think that's that's great that's science yeah we both uh go poke at reality and see who's right what science is I think if we you know the next conversation or more of this conversation is what would be sort of what could we do now besides just say let's keep trying to improve on you know in the car models or you see radical departure what could we do now in the in the current space like what would be good tests of of sort of uh your theory or my theory that we could say you that really lends evidence like you know I'm not as I'm not claiming like uh that if we just keep training them on in the current uh you know with the current architectures although it may be the case you know maybe you know we haven't seen gp5 uh we haven't seen what these things look like with the scaling laws may end up solving the hallucination problem apparently certainly got a lot better uh hallucinations have gone down a lot with with newer models um maybe I'll just sit back and be like let's see um but are there things we could do now experimentally where I I would say look if we fine-tune it you know here's here's some new information uh we're going to give it to the thing and we're going to train it on that information I would expect according to my theory things to work that but according to your theory to work out differently um you know like your case of the the the the siblings and stuff is there something that your theoretical framework would predict uh in the case where you give us certain information uh and you find to it where it would it would you would predict a hallucination or you would predict a kind of a mistake because it doesn't have access to the database whereas I would predict No it should not make these kinds of mistakes yeah we should have that conversation yeah yeah I wish we could do that for the next couple of years that we oh my gosh that would be great okay in in a dream world if anybody wants to uh anybody wants to write a check to support yes uh Daniel's uh next couple years of Graduate School uh then we can focus exclusively on this that would be helpful yes yeah please do you know it's it's it's going be I think it can be astonishing to realize um this is it that this is this is what's doing the all of that stuff yeah and then even more astonishing if you if you buy that that's what's even knowledge aside right that's that sort of at least conversational language use and stuff is like is is enough but yeah I wanted to get into working memory cuz cuz uh I don't think it's a Memory at all I think it's just autoaggressive loop I agree actually you do yes you want to write a thought paper on this because I I I I've been meaning to I want to say working memory is not memory I want to I want to put out a paper basically that now we know that that like I think we that this is a um this is an Insight we now have that autog regression works so well in the linguistic case that there's no reason whatsoever we should posit that anything besides that is happening um and that the reason why you can recall stuff is because from the is because you because it's non marvian because you have to so the brain has to encode the previous few that's the state has to has the sequence and and that all we're doing when we see when you do working memory experiments is we're just tapping into the fact that it's there but it's not for the purposes of retrieval that's where the cognitive science got it wrong they thought this is for retrieval it's not for retrieval it's just the fact that there is activation there's there's uh that in in the brain has to retain that information in order to do next token generation right but it's not so you can then repeat it that's like that's like an EP phenomenon that that happens to work and then stupidly I sorry I mean I I think it's dumb I think they're like because nobody ever gave a good explanation of why you why would you so you can remember a phone number that's what that's that's why we evolved this that's how this this this core function there's a core function of our brain that maybe the core function maybe the core function right of cognitive core function I mean breathing's good too right all that stuff but yes so the cognitive core function is as you can remember a phone number that seem that always seems suspect to me uh like it never was clear why you'd want to go and retrieve like explicitly retriev so I don't think the explicit retrieval is the point at all I think it's a it's a indication it's good evidence that we do retain the previous sequence up to some extent I think the drop off in the drop off in accuracy in retrieval I don't know what it means necessarily because the explicit retrieval is not the point so I don't know if you really are forgetting the sequence uh in terms of like for for next token generation or it's just that the retrieval process this isos retrial proc which is some sort of weird clue like that we're able to do but isn't really the core function may just drop off I don't know and it's an important question for me because can we use all of the is all that cognitive science useful I don't know like the drop off is an important fact but is the the nature of the drop off and the I don't know if that really tells us anything about like how how uh how how the the sequence the context is being updated like you know and being degraded I don't know um I'd like to think that to 50 60 years of work I'd like to think we can go back to it and be like oh now we have insights about how the brain is representing the the sequence for next token Generation Um but I'm not convinced you'd be really interested my kind of Mentor as an undergraduate Dr Andre Erikson he worked on a bunch of the really foundational work on working memory he was older guy his his uh his mentor was his adviser was herb Simon cool and so MIT yeah yeah the kind of of cognitive science yeah um and and so he did a bunch of the foundational work and I remember I talked to him when he was older obviously and he was really disappointed that people had adopted the ideas that him and some of his colleagues came up with so well he said we were scientists we were coming up with uh the best ideas that we CU that's what you have to do as scientist and he said and and I'm proud of that I don't have you you know I don't have but he's not so proud of of the people who came after like who he told they should have done better yeah who he told hey here's the theory and they said oh okay that sounds good and and he was really it's awful he he was really disappointed that in like 30 or 40 years since he had come up with these ideas nobody had come up with anything better and really poked holes in his ideas I think the big problem of recognitive science that's what makes a real scientist by the way that he was disappointed that people weren't dis I love that story I think that's awesome and but but I think the problem with cognitive science um certainly at least the cognitive psychology and of cognitive science uh is that you're you're sipping through a straw like the the kind of data you can collect it's just so constrained and limited that what ends up happening what I think ended up happening is people built theories and sort of like looking for your keys where the light is people built theories out of what they could what they could observe and what can we do we can read people's sequences and they can recall those sequences and you can measure it and it's highly measurable and we can like you know there's there's empirical data you know this is the the behaviorists uh kind of uh impact you know don't tell me about your high fluen abstract theories what can we measure and and and they were constrained by what they can measure and you end up with something like there's a core function of the brain to retrieve because we can see that people can retrieve but maybe it's just because that's how you're able what something you're able to measure is retrieval and maybe that's not even the core function at all that's that's sort of my claim and I think maybe this is true for a lot of cognitive psychology that you end up unfortunately because of the limitations in in the kind of data you can collect you end up building theories out of uh that very very narrow limited kind of data and I you know I don't know that there was a better solution I think now we've got computation models that do that are able to do this stuff it's a different way of doing you know cogni science always was supposed to also be doing but the computational stuff work mhm you know the architectures the the AI architectures good old AI um all of it uh uh to some extent didn't quite produced The connectionism Parallel PDP whatever ended up becoming neural networks turns out to have worked um to to do some some very significant portion of of of what we call cognition um and you know I guess the argument could be like that's okay so you know that that was necessary everybody had to pursue these different but I I don't think uh but I guess what I would say is like I think there's textbooks filled with which are probably including my own textbook filled with wrong information uh making claims about boxes that maybe don't exist um in these you know so long-term memory short-term memory working or was was working memory as sort of a memory system I think that needs updating I think maybe the the conclusions that were drawn need to be updated um based on new new facts as have come to light uh new come to late um that that call into question whether these these were these were inaccurate were actually mistakes made um I don't know yeah yeah I think um yeah I think there are boxes but because large language models are a giant Universal function approximator and because they have all of this data they managed to simulate those boxes within the large language model and and so right our debate is whether it's one box or multiple boxes but you may agree that the particular boxes that of science has arrived that may not may not be% but but I think uh you know we large language models are really smart and they're very impressive but if you gave me the brain a size of a warehouse and gave me a thousand years to read the entire internet I could beat the pants off a large language model um so but they're going to get more they are getting more efficient all the time they're some spectacular much much I mean compared to the a number of parameters you needed uh you know a year ago um and and some of these lighter um by the way uh open source models that are like a few billion parameters uh they're they're much smaller and they're they're beating benchmarks so maybe we'll get to much more efficient and the training regimen also it hasn't been much of a an area of research I mean at least I don't know what's going on on the on on industry side I think there's potentially a lot of work that could be done um in training these things in a more sequential way like right now you don't we don't do it the way we do with humans maybe they should start out with simple stuff and then build up to more complex stuff maybe that that would lead to uh you know a faster and and you don't need as much data but but computer is cheap enough and people don't really care um like if we get to that model they only care about inference uh you know if everybody just wants to get to the model that you know the best model and they don't really care about you know they're throwing ridiculous amounts of money and and compute and then once it's done it's done uh and they're like okay uh you know they're not worrying about right now they're just scale scale scale scale um and and but in terms of explaining and trying to replicate what humans do which doesn't require you know what what are the equivalencies like thousand hundreds of thousands of years you would need to spend as human being going through all this data and and of course that's unrealistic but from a scientific standpoint maybe the engineers don't care um but thinking about how people are able to learn so much and get so good with so so much less data of course is a critical question um both for just language use but knowledge all of this stuff um and again maybe you would say that it's because we do it completely differently I would say maybe not difference in in you know qu qualitative just a quantitative difference or or or you know a better model with a better training regimen maybe we could end up doing something like we see with humans and there's also we don't know what genetics is doing we don't know what the structure is like how much knowledge knowledge may be in there to some extent I mean you I don't want to quote chsky too hard on this but because I I think he's a guy I think we he's wrong I think I think grammar syntax it's wrong it's like now we know you could do you could do you could do language production without syntax you don't need it's not to be explicit it's not in there um it's just not in there it's implicit but that's that's EP phenomenal whatever we can get into that but he is right in the sense that there's not enough data uh children don't have enough data to learn the rules so there's some sense in which perhaps these rules remember they're just weights right whatever is when when the model's trained you're just chaining the weights maybe in the process of you know genetics plus developmental environment plus you know stuff that's happening in the womb and all this stuff may give you lots of scaffolding that makes you ripe for for language that's that's not that's before you ever truly encounter language um so it's not um it's it's it's it there's there's room for for lot lot of other stuff besides what we we think of classically as as training a model there's a meta point to be made here about AI research which is that all of this work may already be going on but AI research currently because it's the biggest economic thing in the world is in the same place that Finance research or defense research is which is the really good stuff doesn't get published because you don't want other people to know about the really good stuff right that that's my deep-seated hope and fear at the same time like uh maybe we'll never know about it may it's just like boom my model can run on a thumb drive and I'm not telling you how it's like and it's able to do all the stuff and you know you can put it in an edge and and it works and we'll never find out and turns out you solved like the problem of cognition you know in Deep Way like that that tells us about humanity and we're like we're never going to find out because it makes enough money and and and and absolutely and and as much as open source and and and all of AI and and the progress has been a of the fact that there's a tremendous amount of of sharing and collaboration that the open source uh kind of ethos has been huge and and critical um but let's face it open AI is not open uh and then we don't know what their models are we don't know how they work um we can only try to draw certain very coarse inferences about their scale uh you you could try to follow as some people I know do like GPU orders how many h100s are shipping and things like that but you don't actually find out what what tricks they're using um and I agree with you that they very well maybe um these kinds of things are happening and insights are are taking place but but science for Sciences sake and basic research you know I don't there's not really an incentive at a motive for the the industry to care how humans do it on our wet wear which has all these other constraints and and and doesn't have to operate in the same under the same conditions like maybe they just don't care yeah um if there is economic incentive that would be other concerns that I would have but oh well let's not get into sure but you know unless they think there's tricks there's secrets that the brain has then they want to steal them but frankly their job is not to figure out how the brain works um and and to and therefore if there's a Direction they can go in that that that just seems to yield fruit that doesn't reflect how their brains doing it they're not they shouldn't care you know they they they you know to use the modern parland they're they're beholden to their shareholders right uh although is not public traded but they're the organism they're beholden to Microsoft they're they are they're very much beholden to Microsoft and M other investors that's true um and they're beholding to their organization right they they have their they are they got to stay up front the top of the pack and they have to do whatever they and they have to put all their resource into that they can't be worrying about well is this how the brain does it but what about you they they don't have time for that so uh I hope that you know that there's that uh a um people whose whose concerns are maybe different than just engineering the best uh most efficient and economically viable model uh have an opportunity to to to Plum these depths um and and I hope that they're they're able to do that um and and I hope also perhaps that whatever happens in an industry you know can percolate out at some level there's a big concern uh I think we all should have then that science and and acade you know Academia itself is just is falling off the map in the sense that like the progress is all happening in Industry um and that means that the insights may be closed off from us uh you know because they're privatized and they're not asking these kinds of questions that I think uh basic research people interested in Neuroscience for example uh would want to ask and so what's the solution to that well you know I guess more Public Funding that's not exactly the seems to be the climate right now and and Public Funding isn't really funding these types of things either you you and I both know Public Funding and Neuroscience is extremely extremely clinically focused um extremely clinically focused which is understandable so I should say I'm not against the people that are doing this because the people that are giving out this funding is the National Institute of Health and it's completely reasonable for the National Institute of Health to be clinically just a quick side so so Daniel and on on a we're on a grant uh we won't get too much into it but it's it looks it's it's trying to predict uh or or to detect um psychoactive substances that are emerging and stuff and and there's somebody from the nah who sits in every meeting and often her her her role is to say okay what are the health implications what are the public health implications of this and that's and an inches mandate good and and and I applaud that I appla that if my tax dollars are going to the National Institute of Health I want them to be Health focused correct however basic research as we all know yeah has yielded you know extraordinary results that have had health implications and and you know human human focused implications and it it's it seems like this is a moment in time where there's really really some very powerful important insights into you know the nature of language and probably the you know the nature of cognition and all of that um and I would I would hope and I I you know personally you know uh I I guess Advocate if for lack of a better word um that basic research and and so research for the sake of understanding and human human cognition um in this particular time period should should really be it has to be boosted like we can't let industry just walk away with this thing uh and and not learn what what's POS you know what we could possibly learn because we don't know what what are the possible implications right um for for health for for you know for for uh understanding ourselves understanding how the brain works there does this have implications for here's a good one right Alzheimer's and and memory loss I have some things to say about that um and I don't know you know I don't know if they're true or not but that are are uh strongly influenced by these conversations uh the things we're talking about like like autoaggression like and the and the memory and the the the the the fact that you need to maintain that sequence I think there there may be some very core insights uh if we understand the brain this way and if we could substantiate that the brain operates this way that could have real implications as to understanding the nature of of memory loss what is the what is sort of the computational basis of of dementia what's going on um at at sort of this level um can do we have some speciic things to say now uh on the basis of that if if we have a theory now of what how cognition operates and it involves in this case uh the ability to maintain Ain a certain um uh uh Fidelity you know a certain degree of fidelity of uh the the previous uh you know sequence that happened recently in memory um if we if we kind of buy that the the the architectures um that now work computationally in say llms may actually real yield some real insight into how it works in human brains and maybe we we'll have something substantive to say about something like dimension um and it's not nor pharmacological it's not saying that you know it's not it's not giving cellular molecular basis for it it's in some it's computational but that doesn't mean it wouldn't have broad implications for how once you understand it this way if it gives you insight to understanding what's going on that could of course have implications you know let's say it's about brain regions and how those brain regions are talking to each other where is the breakdown taking place that could of course dictate and and and form uh how you might design a drug to Target uh you know these deficits so um I I think uh the health implications the sort of human focused implications it's not just knowledge for knowledge's sake uh it's not just sort of computational philosophy I which personally I I think is important uh you know as a species we do want to understand ourselves too but I think uh it behooves us as uh you know as a society even if if concrete results and and sort of health benefits and um societal benefits are are what we're we're throwing our money into and I think that is you know very reasonable to say that that's what we're going to put tax dollars behind I think that um this this certainly could yield a this these kinds of um investigations are are potentially very important yeah and I think it's worth mentioning and this is sort of a central point in my dissertation because in my dissertation I'm using large language models to help people build better Neuroscience the yes and and it's and it's worth mentioning that uh neuroscience research and all scientific research but Neuroscience research works like a funnel where you have what are called computational neuroscientists who come up with an idea and they test it on data that's already available so they just say here's the state of how these neurons connect to these neurons and I'm going to run my theory and see if my theory plays out how I expect it to there and then some percentage of theories get knocked out at that stage um but the good theories make it to the experimentalist and the experimentalist have rats and flies and all sorts of other things and they say all right well we're going to test this theory in a more experimental way and here's how you would falsify this Theory and we're going to do an experiment and then some of the experiments don't work out how the theory predicts and so you throw out some amount of uh things at that point and then the experimentalist pass things off to the clinical neuroscientist and the clinical neuroscientists do further experiments but experiments that are more focused on all right well if the brain works this way then it means Alzheimer's works this way and it means if we do this we should be able to help Alzheimer's and so they'll do some sort of experimentation really focus more on an application like that and some of those things don't work out how the theory predicts so more things get thrown out then and then finally you have sort of clinical trials which the FDA is involved with and that's where things are really really tested rigorously before they're used on humans and some percentage of things are thrown out then and so you have something like a funnel where theoretical ideas about the brain get thrown out at each stage and by some napkin math estimation I've done looking at sort of the stats of how many theories make it to experiments and how many experiments make it to clinical uh neuroscientists and how many of those make it to clinical trials and how many of those actually make it into real products that help people I estimate that if we're able to build theories so do this sort of fundamental work if we're able to build theories that are 5% more robust then we're able computational like on the comput on the computational side then we're able to double the amount of effective clinical trials cool that's um so that's huge that's that's having the time till we find a cure for Alzheimer's and Parkinson's and all of these other really important things but the work that the computational neuroscientist is doing at the beginning of this process he's not necessarily talking about anything clinical he's not talking about Alzheimer's he's not talking about Parkinson's it sounds a lot more like the conversation we've been having and so that receives less funding but but actually if you give a small amount of funding here the other nice thing about computational science it's cheap you don't have to buy equipment so a very small amount of funding here can have outs sheap guys just send checks to can but a very small amount of funding here can have an outsized impact down the road I think it's a great point and but I I I want to so there's there's something in that about this moment in time um where the computational theories so historically and I don't want to throw shade on the computational approach because because absolutely necessary and as we've discussed you don't know where the insights are going to come from however historically this most of computational Neuroscience of fit to and these models of spiking neurons and all stuff they didn't do much they tried to replicate certain kinds of uh sort of low-level dynamics that tend that we observed in the brain the models now some of them anyway right these at least like the L models but so you know general in general neural networks they actually do the thing they do the thing uh the behavioral thing they go all the way in some ways to behavior and so it's much easier in some ways to to make the case now that these computational models have first of all that they have these implications potentially but going back to your sort of point of like the robustness and and and the likelihood of success it's I think there's a an environment now in which your your likelihood of success should be higher if you're working within the domain where you can actually see does it do the thing like can we get like I think we're going to get to the point probably fairly soon with robotics plus plus AI um where you're going to be able to test these fruit fly models in a synthetic fruit fly uh you know it's going to do the thing that fruit flies are supposed to do uh or or you can you know or you can leion it in a ways that uh simulate allion uh in a fruit fly and so as the models get more and more powerful as they as they surely are becoming uh I think the argument becomes much stronger uh for investing in on the basic research side uh you know computational models not with a clinical end in mind but at least a behavioral end in mind something that's close not just simulating some process deep within the bowels of the brain but that actually in form some outcome that in some ways looks similar to the outcomes we care about things like know whatever navigation whatever it is your your fuit flly or memory learning whatever the the fruit flies do um or the people do uh I think I so I think this should be a golden age MH uh and and again we we need to invest in for it to be a golden age but golden age of of computational Neuroscience computational research with possible with these kinds of possible um human focused human positive outcomes and and there's things that have happened just in the past five years that I think have gotten computational Neuroscience to this point where it's a really exciting time to be in it where uh we've gotten the complete connectum of the adult fruit fly every single neuron how it connects to every single other neuron that's huge like you should do a dissertation I'm doing a dissertation on that we've got a complete map of every single cell in the mouse brain right in the past 5 years right we've got the first Clinic clinical tool that us uses these connectomes human connectomes to to sort of assess all right where is the damage to humans we've gotten uh we've got a huge huge project working on getting the complete mouse connecto that uh I expect to to wrap up within the next 5 or 10 years we've got you need to finish your dissertation before that though just so you know yeah yeah yeah yeah I'm working on uh we've got more genetic information about the brain than we've ever had before I mean the amount of data that we've got that you can test your on and sort of have a synthetic fruit fly is is enormous so uh this is an exciting moment to be in computational Neuroscience yes and and and and just the the just the additional factor that at least some forms of computation have have really proven to be um like models yeah it seems of of the real thing I think should put the sale put the wind in the sales uh of of computationalism in general not necessarily the the classic computationalism of you know of what cognitive science historically did but closer to uh you know noal networks where you're you're You're Building Systems that that can um I I think one thing we've you know get the human the human conceptualization can sometimes be uh a little bit of a uh a red a red herring or a a Garden Path um I think that's we don't need to be able to you know it's not about conceptualizing it externally um it we may just be able to do it building smarter machines in the way that we now know to do data plus architecture and and that's not sort of the classical cognitive science perspective of computational it wasn't about it was about theorizing about what the architecture was now we were like it's I think in some ways it's healthy shift is to say um let's just try to finesse the architectures yeah build something that works build something that works um and then figure out after the fact okay what does that tell us like in the case of LMS in some ways it wasn't because I don't think people said hm uh autoaggression seems to be the way humans do it and I'm proposing that and therefore let's build it this way instead it was what can we get an noral Network to do predict the next output and then what if we do it this way boom it worked lo and behold that ends up being pretty very very similar to what you expect people to do we kind of think sequentially and and we talk sequentially um so yeah maybe you know get get the uh the smart people have to take a step back like don't be too smart don't be too clever about thinking uh that our brains are going to sort it out uh theoretically and instead really kind of lean in on on on sort of computationalism uh you know as as as the approach as almost like engineer our weight to these Solutions but with a basic research kind of me ality um yeah yeah thanks that was a a really cool and interesting conversation and and and uh I said there's something really kind of meta about this which is that in by in the process of of talking about stuff I'm sort of running these autoagressive models and reasoning through um something we haven't talked about you know sort of like the the the Chain of Thought and and this kind of reasoning processes how that and it lands you in a different spot and you know coming back to your point I have a different I have a different model Now by virtue of of these conversations and it's and it and uh so the there is clearly this Dynamic that's happening you know even though I think it's one system but it's one system in in embedded in uh in the world and and and in a conversational context that's going to get fed back like next time we talk you know even if it's you know a week from now we're not going to be midc conversation but I'm going to talk about this and not just with you I'm gonna I'm gonna think about this differently going forward um so there's this very cool kind of process of certainly informing and updating the model through this conversation but conversation as thought is sort of one of the key foundations of like this whole perspective is that that's what we're doing um you it's not all in there the information's not in there it's not in the model um it has to do this stuff get it out of this model get it out of itself work through things and then that ends up getting fed back into the model to retrain itself so it's all very odd but it's sort of a wonderful this this was a sort of a wonderful kind of a demonstration for me of that process yeah absolutely and this is the really cool thing about being a neuroscientist is you get to use the brain to study the brain and and that's a unique place to be well you can find me on Twitter and substack and then I have my own website uh I'm also on on uh Blue Sky because I know people are leaving Twitter and drov right now so uh now I'll show my age my you know my socials are uh much less developed uh but I do have a website uh that's kind of rudimentary on baron. a um I have a substack which I uh have to populate with more stuff um but uh if you check it out and I know you're visiting that'll encourage me to uh to do a better job on the social side so uh please encourage me to do that oh like And subscribe yes this is an awesome channel uh he's having lots of conversations like these on here oh yeah like and sub I love this channel just I I have to skip over the the the the interviews with me uh I don't look at those but uh I love to watch the other stuff it's it's a great channel so like And subscribe absolutely