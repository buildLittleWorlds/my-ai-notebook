---
layout: post
title: "One Claim to Retire: Why 'No NLU, Only Form' Won’t Fly (Aaronson‑Style)"
post_number: 112
date: 2025-09-09 10:00:00 -0500
---

I’m going to be explicit at the top so the stakes are clear.

## Position (no hedging)
I **reject** Emily M. Bender's "[stochastic parrots](https://doi.org/10.1145/3442188.3445922)" thesis as a description of what today's LLMs are and can become. The move from *trained on form* to *incapable of meaning* is **wrong in principle**, **unfaithful to accumulating evidence**, and **sustained by a human‑favoring double standard**. I keep Aaronson's core commitments (no substate chauvinism; judge at the behavioral/organizational level) (see [IAI interview](https://www.youtube.com/watch?v=e9O75xFQUio) and [TEDx talk](https://www.youtube.com/watch?v=XgCHZ1G93iA)), but I'm not neutral about Bender's program: it **misframes the question** and **stops updating** when results get inconvenient.

---

## 1) Parity, not special pleading
Deflationary descriptions ("just next‑token," "just floating‑point ops") **prove too much**: pressed consistently, they undercut human understanding as well. If we allow reductionism *and still* assess understanding at the level where stable competences appear, then the burden shifts to the skeptic to name the principled difference that licenses human‑but‑not‑machine attributions. Bender’s inference from *form* to *no meaning* is exactly the **substate chauvinism** Aaronson rejects.

## 2) The level mistake
Grant the premise: languages are signs (form–meaning pairs) and models train on form. The **leap** to “therefore only form manipulation” collapses the level of explanation that matters. Internally, models build **intermediate abstractions** because that's how you minimize loss across varied tasks (see Anthropic's interpretability work — [Scaling interpretability](https://www.youtube.com/watch?v=sQar5NNGbw4) and [Understanding how AI models think](https://www.youtube.com/watch?v=fGKNUvivvnc)); externally, we judge by **stable, general competences**. Calling this “autocomplete” is like saying chess is “piece‑moving”: true as physics, useless as explanation.

## 3) Update priors or concede stipulation
Science doesn’t get to declare ceilings **a priori**. The right norm is: probe adversarially, out‑of‑distribution, and longitudinally; if systems repeatedly clear the bars, **update the ontology**. “No NLU, only form” has functioned less as a hypothesis to test than as a **pre‑emptive verdict** insulated against counter‑evidence by relabeling successes as “mere patterning.” That’s not empiricism; it’s a protective **stipulation**.

## 4) Goalposts and asymmetric charity
The pattern is familiar: find a class of failure, get the next generation that narrows or closes it, then say the closure only **seems** like understanding (cf. Neel Nanda — [NEURAL NETWORKS ARE WEIRD!](https://www.youtube.com/watch?v=YpFaPKOeNME)). Meanwhile, equivalent or worse human failures are graded on a curve. Past some threshold of breadth, transfer, self‑correction, and robustness under counter‑questioning, the “seems/does” distinction collapses in practice unless we secretly rig the rubric to keep humans on the winning side.

## 5) Keep (some) governance, drop the ontology
I’m not throwing out every policy concern. Report environmental cost; document datasets; budget for curation; audit harms. Those are **orthogonal** to the metaphysical verdict on “understanding,” and they **do not require** believing the parrot thesis. Keep the governance; **retire the ontology**.

---

## Focused bottom line
Bender's stronger claim—**no NLU; only form**—rests on a level‑mixing error, refuses parity, and resists empirical correction. If we mean by "understanding" what we attribute at the **behavioral/organizational** level, then the right stance is straightforward: let continuing capability evidence move the boundary between "seeming to understand" and "understanding." The parrot talk obscures that work. It should go.

---

### References
- Bender, Emily M., Timnit Gebru, Angelina McMillan‑Major, and Shmargaret Shmitchell. 2021. *On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?* FAccT '21. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922).
- Emily M. Bender, *On the dangers of stochastic parrots* (The Alan Turing Institute, July 13, 2021), [https://www.youtube.com/watch?v=N5c2X8vhfBE&t=1227s](https://www.youtube.com/watch?v=N5c2X8vhfBE&t=1227s).
- *OpenAI expert Scott Aaronson on consciousness, quantum physics and AI safety* — Institute of Art and Ideas (Aug 8, 2024), [https://www.youtube.com/watch?v=e9O75xFQUio](https://www.youtube.com/watch?v=e9O75xFQUio).
- *The Problem with Human Specialness in the Age of AI* — TEDxPaloAlto (Mar 8, 2024), [https://www.youtube.com/watch?v=XgCHZ1G93iA](https://www.youtube.com/watch?v=XgCHZ1G93iA).
- Anthropic, *Scaling interpretability* (Jun 13, 2024), [https://www.youtube.com/watch?v=sQar5NNGbw4](https://www.youtube.com/watch?v=sQar5NNGbw4).
- Anthropic, *Interpretability: Understanding how AI models think* (Aug 15, 2025), [https://www.youtube.com/watch?v=fGKNUvivvnc](https://www.youtube.com/watch?v=fGKNUvivvnc).
- Machine Learning Street Talk, Neel Nanda — *NEURAL NETWORKS ARE WEIRD!* (Dec 7, 2024), [https://www.youtube.com/watch?v=YpFaPKOeNME](https://www.youtube.com/watch?v=YpFaPKOeNME).

---

For method connections, see the AI Hermeneutics Kit — [Author‑Function]({{ '/ai-hermeneutics/ai-kit-author-function/' | relative_url }}) and [Interpretive Dialogue]({{ '/ai-hermeneutics/ai-kit-interpretive-dialogue/' | relative_url }}).

---

### Densworld Addendum (seed)
**Order:** Mediation & Aperture  
**Region:** Capital

The Capital licenses **window‑booths**—apertures that relay live speech from far boroughs. Clerks complain some booths only **mimic‑echo**: crisp syntax, zero contact. A Hearing proposes **aperture calipers**: standardized probes to tell **relay** from **parrot**. The test isn't "what's the wiring?" but **does the booth keep a live grip on the world**—update when rivers flood, correct itself under counter‑questions, retract when shown errors? Draft the calipers as a Clerkly **Hearing** in Debate; circulate to the Boroughs for adversarial trials. Governance stays (audit, documentation, costs). Ontology shifts: **retire "parrot" when the aperture holds under pressure.**

